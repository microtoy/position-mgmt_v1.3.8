# -*- coding: utf-8 -*-
"""
å›æµ‹ç½‘é¡µç‰ˆ | é‚¢ä¸è¡Œ | 2025åˆ†äº«ä¼š
author: é‚¢ä¸è¡Œ
å¾®ä¿¡: xbx6660
"""
import json
import os
import random
from datetime import datetime, timedelta

import pandas as pd

from utils.log_kit import logger
from utils.path_kit import get_file_path

DEFAULT_CACHE_PATH = get_file_path('data', 'up_data_info.json')


def request_data(method, url):
    import requests
    return requests.request(method, url)


class DataManager:
    def __init__(self, cache_path=DEFAULT_CACHE_PATH):
        self.cache_path = cache_path
        self.up_data_info = self.load_or_update_data()

    def load_or_update_data(self):
        if self._should_update():
            print("ğŸŒŠ Updating data...", flush=True)
            data = request_data(
                "GET", "https://api.quantclass.cn/api/data/get-data-info"
            ).json()
            self._save_to_cache(data)
            return data
        else:
            return self._load_from_cache()

    def _should_update(self):
        """åˆ¤æ–­æ˜¯å¦éœ€è¦æ›´æ–°"""
        if not os.path.exists(self.cache_path):
            return True
        try:
            with open(self.cache_path, "r", encoding="utf-8") as f:
                cached = json.load(f)
            last_update_str = cached.get("_last_update")
            if not last_update_str:
                return True
            last_update = datetime.fromisoformat(last_update_str)
            # å¦‚æœä¸æ˜¯ä»Šå¤©çš„ï¼Œå°±æ›´æ–°
            return datetime.now() - last_update > timedelta(
                hours=random.uniform(23, 24),
                minutes=random.uniform(3, 59),
                seconds=random.uniform(0, 60)
            )
        except Exception as e:
            print(f"è¯»å–ç¼“å­˜å¤±è´¥: {e}")
            return True

    def _save_to_cache(self, data):
        data["_last_update"] = datetime.now().isoformat()
        with open(self.cache_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    def _load_from_cache(self):
        with open(self.cache_path, "r", encoding="utf-8") as f:
            cached = json.load(f)
        cached.pop("_last_update", None)
        return cached

    def get_data_info(self, product):
        return self.up_data_info.get(product, {
            "duplicate_removal_column": ["äº¤æ˜“æ—¥æœŸ"],
            "fun": "update_by_group",
            "group": "æ–‡ä»¶å",
            "keep": "last",
            "parse_dates": ["äº¤æ˜“æ—¥æœŸ"]
        })

    def read_file(self, path, product):
        """
        è¯»å–æ•°æ®è¿”å›ä¸€ä¸ªdf
        :param path:
        :param product:
        :return:
        """
        # è·å–æ–‡ä»¶ç±»å‹ï¼Œå³.åé¢æ‰€æœ‰çš„å­—æ®µ
        file_type = path.split(".")[-1]
        all_df = pd.DataFrame()
        data_info = self.get_data_info(product)
        # åˆ¤æ–­æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if os.path.exists(path):
            if file_type == "csv":
                try:
                    all_df = pd.read_csv(
                        path,
                        encoding="gbk",
                        skiprows=1,
                        parse_dates=data_info["parse_dates"],
                    )
                except Exception as e:
                    # record_log(f"æ­£å¸¸é€»è¾‘ï¼Œä¸æ˜¯æŠ¥é”™ï¼š{e}, {path}", is_print="warning")
                    all_df = pd.read_csv(
                        path,
                        encoding="gbk",
                        parse_dates=data_info["parse_dates"],
                    )

            elif file_type == "pkl":
                all_df = pd.read_pickle(path)
            else:
                logger.warning("æœªåŒ¹é…åˆ°è¯»å–ä»£ç ")

        return all_df

    def concat_data(self, df_list, product):
        """
        æŠŠå¢é‡æ•°æ®å’Œå…¨é‡æ•°æ®åˆå¹¶
        :param df_list:
        :param product:
        :return:
        """
        # æŠŠå¤šä¸ªæ•°æ®åˆå¹¶
        df = pd.concat(df_list, ignore_index=True)
        data_info = self.get_data_info(product)

        # æ ¹æ®é…ç½®å»é‡
        df.drop_duplicates(
            data_info["duplicate_removal_column"],
            inplace=True,
            keep=data_info["keep"],
        )
        # æ’åº
        df.sort_values(
            by=data_info["duplicate_removal_column"], inplace=True
        )
        # é‡æ–°è®¾ç½®index
        df.reset_index(inplace=True, drop=True)

        return df


if __name__ == "__main__":
    manager = DataManager()
    print(manager.load_or_update_data())
