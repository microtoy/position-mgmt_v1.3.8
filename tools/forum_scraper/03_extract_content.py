import datetime
import asyncio
import sys
import os
import json
import random
import time
import re
import requests
from playwright.async_api import async_playwright
from util_stealth import apply_stealth, random_sleep, human_scroll, get_random_ua

# --- Configuration ---
with open("config.json", "r", encoding="utf-8") as f:
    config = json.load(f)

LINKS_FILE = "links.json"
PROGRESS_FILE = "progress.json"
STATE_FILE = "state.json"
OUTPUT_DIR = "downloaded_pdfs" 
PROFILE_DIR = "browser_profile"
FIXED_UA = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"

# --- Safety Settings ---
DAILY_LIMIT_MIN = 50  # æ¯æ—¥æœ€å°‘ 50 ç¯‡
DAILY_LIMIT_MAX = 80  # æ¯æ—¥æœ€å¤š 80 ç¯‡
HOURLY_LIMIT = 10     # æ¯å°æ—¶æœ€å¤š 10 ç¯‡
NIGHT_START = 23    # å¤œé—´ä¼‘æ¯å¼€å§‹æ—¶é—´ï¼ˆ23ç‚¹ï¼‰
NIGHT_END = 7       # å¤œé—´ä¼‘æ¯ç»“æŸæ—¶é—´
DAILY_COUNT_FILE = "daily_count.json"  # è®°å½•æ¯æ—¥è®¿é—®é‡
HOURLY_COUNT_FILE = "hourly_count.json" # è®°å½•æ¯å°æ—¶è®¿é—®é‡

# æ—¶æ®µé…ç½®ï¼šæ¨¡æ‹Ÿäººç±»å­¦ä¹ èŠ‚å¥
# æ—©é—´(7-12): æ´»è·ƒå­¦ä¹ ï¼Œé—´éš”çŸ­
# åˆé—´(12-14): åˆä¼‘ï¼Œæš‚åœæˆ–ææ…¢
# ä¸‹åˆ(14-18): æ´»è·ƒå­¦ä¹ 
# æ™šé—´(18-23): è½»åº¦å­¦ä¹ ï¼Œé—´éš”é•¿
TIME_SLOTS = {
    "morning": {"hours": range(7, 12), "min_sleep": 45, "max_sleep": 90},      # æ—©é—´ï¼šå¢åŠ é—´éš”
    "lunch": {"hours": range(12, 14), "min_sleep": 300, "max_sleep": 600},     # åˆä¼‘ï¼šåŸºæœ¬æš‚åœ
    "afternoon": {"hours": range(14, 18), "min_sleep": 50, "max_sleep": 100},  # ä¸‹åˆï¼šå¢åŠ é—´éš”
    "evening": {"hours": range(18, 23), "min_sleep": 70, "max_sleep": 150},    # æ™šé—´ï¼šæ›´æ…¢
}

if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)

# --- Define Logging ---
import logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("scraper_pdf_final.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# --- WeCom Notification ---
WECOM_WEBHOOK = config.get("wecom_webhook", "")
WECOM_WEBHOOK_ERROR = config.get("wecom_webhook_error", "")

def send_wecom_alert(title: str, content: str, is_error: bool = False):
    """å‘é€ä¼ä¸šå¾®ä¿¡ç¾¤é€šçŸ¥ï¼Œis_error=True æ—¶ä½¿ç”¨é”™è¯¯ä¸“ç”¨ Webhook"""
    webhook = WECOM_WEBHOOK_ERROR if is_error else WECOM_WEBHOOK
    if not webhook:
        logger.warning("æœªé…ç½®ä¼ä¸šå¾®ä¿¡ Webhookï¼Œè·³è¿‡é€šçŸ¥")
        return
    
    try:
        # æˆåŠŸç”¨ç»¿è‰²å‹¾ï¼Œå¤±è´¥ç”¨çº¢è‰²è­¦å‘Š
        icon = "âš ï¸" if is_error else "âœ…"
        payload = {
            "msgtype": "markdown",
            "markdown": {
                "content": f"### {icon} {title}\n{content}"
            }
        }
        resp = requests.post(webhook, json=payload, timeout=10)
        if resp.status_code == 200:
            logger.info("âœ… ä¼ä¸šå¾®ä¿¡é€šçŸ¥å‘é€æˆåŠŸ")
        else:
            logger.warning(f"ä¼ä¸šå¾®ä¿¡é€šçŸ¥å‘é€å¤±è´¥: {resp.text}")
    except Exception as e:
        logger.error(f"ä¼ä¸šå¾®ä¿¡é€šçŸ¥å¼‚å¸¸: {e}")

# --- Progress Management ---
def load_progress():
    if not os.path.exists(PROGRESS_FILE):
        return {"processed": [], "queue": []}
    with open(PROGRESS_FILE, "r", encoding="utf-8") as f:
        data = json.load(f)
        if "queue" in data and data["queue"] and len(data["queue"]) > 0:
            if isinstance(data["queue"][0], str):
                 data["queue"] = [(url, False) for url in data["queue"]]
        return data

def save_progress(processed, queue):
    with open(PROGRESS_FILE, "w", encoding="utf-8") as f:
        json.dump({"processed": list(processed), "queue": queue}, f, indent=2, ensure_ascii=False)

async def get_clean_title(page):
    """Wait for a real title to appear, checking both document.title and DOM elements."""
    for i in range(20): # Increase wait to 20s
        # 1. Check document.title
        t = await page.title()
        t = t.replace("- é‡åŒ–å°è®ºå›", "").strip()
        t = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', t)
        
        # 2. Check DOM for title if document.title looks generic
        if not t or any(kw in t for kw in ["ä¸»é¢˜è¯¦æƒ…é¡µ", "é‡åŒ–å°è®ºå›", "Loading", "æ­£åœ¨åŠ è½½", "æœ€æ–°å›å¤"]):
            try:
                # å°è¯•å¤šç§é€‰æ‹©å™¨è·å–çœŸå®æ ‡é¢˜
                dom_title = await page.evaluate("""() => {
                    // æŒ‰ä¼˜å…ˆçº§å°è¯•å¤šä¸ªé€‰æ‹©å™¨
                    const selectors = [
                        '.thread-title',
                        '.article-title', 
                        '.post-title',
                        '.topic-title',
                        'h1.title',
                        'h1',
                        '.content-header h1',
                        '.main-content h1'
                    ];
                    for (const sel of selectors) {
                        const el = document.querySelector(sel);
                        if (el && el.textContent.trim().length > 2) {
                            const text = el.textContent.trim();
                            // æ’é™¤é€šç”¨æ ‡é¢˜
                            if (!text.includes('ä¸»é¢˜è¯¦æƒ…é¡µ') && !text.includes('æœ€æ–°å›å¤')) {
                                return text;
                            }
                        }
                    }
                    return "";
                }""")
                if dom_title:
                    t = dom_title.strip()
            except:
                pass

        # 3. Validation - æ’é™¤æ›´å¤šé€šç”¨æ ‡é¢˜
        generic_titles = ["ä¸»é¢˜è¯¦æƒ…é¡µ", "é‡åŒ–å°è®ºå›", "Loading", "æ­£åœ¨åŠ è½½", "æœ€æ–°å›å¤", "é¦–é¡µ"]
        if t and all(kw not in t for kw in generic_titles):
            if len(t) > 2:
                logger.info(f"  -> Found clean title: {t}")
                return t
        
        if i % 2 == 0: logger.info(f"  -> Waiting for title... (Current: {t})")
        await asyncio.sleep(1)
    
    # Fallback: å°è¯•ä»é¡µé¢å†…å®¹æå–æ ‡é¢˜
    final_t = await page.title()
    final_t = final_t.replace("- é‡åŒ–å°è®ºå›", "").strip()
    
    # æœ€åå°è¯•ï¼šä»æ–‡ç« å†…å®¹çš„ç¬¬ä¸€è¡Œæå–
    try:
        first_line = await page.evaluate("""() => {
            const article = document.querySelector('.article-cont') || document.querySelector('.vditor-reset');
            if (article) {
                const firstP = article.querySelector('p, h1, h2, h3');
                if (firstP) return firstP.textContent.trim().substring(0, 50);
            }
            return "";
        }""")
        if first_line and len(first_line) > 5:
            return first_line
    except:
        pass
    
    return final_t

async def process_content():
    progress = load_progress()
    processed_urls = set(progress["processed"])
    work_queue = progress["queue"]
    
    if not work_queue and os.path.exists(LINKS_FILE):
        with open(LINKS_FILE, "r", encoding="utf-8") as f:
            all_links = json.load(f)
            count = 0
            for link in all_links:
                if link not in processed_urls:
                    work_queue.append((link, False))
                    count += 1
        logger.info(f"Reloaded {count} items from links.json")


    consecutive_failures = 0
    last_short_break = time.time() # Timer for 30m breaks
    
    async with async_playwright() as p:
        # STRATEGY: Persistent Browser Profile for maximum durability and realistic fingerprint.
        logger.info(f"Initializing PDF Scraper (Mode: Persistent Context, Profile: {PROFILE_DIR})...")
        
        # Use absolute path for PROFILE_DIR to ensure consistency across CWDs
        abs_profile_path = os.path.abspath(PROFILE_DIR)
        
        context = await p.chromium.launch_persistent_context(
            user_data_dir=abs_profile_path,
            headless=True,
            user_agent=FIXED_UA,
            # Increase viewport width to ensure tables fit!
            viewport={"width": 1600, "height": 1200}
        )
        
        page = context.pages[0] if context.pages else await context.new_page()
        await apply_stealth(page)

        # [Addition] Inject cookies AND LocalStorage from state.json
        state_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), STATE_FILE)
        if os.path.exists(state_path):
            try:
                with open(state_path, "r", encoding="utf-8") as f:
                    state_storage = json.load(f)
                    
                    # 1. Inject Cookies
                    if "cookies" in state_storage:
                        await context.add_cookies(state_storage["cookies"])
                        logger.info(f"  -> Injected {len(state_storage['cookies'])} cookies")
                    
                    # 2. Inject LocalStorage via Init Script (Deep Restoration)
                    if "origins" in state_storage:
                        for origin_data in state_storage["origins"]:
                            origin = origin_data.get("origin")
                            ls_items = origin_data.get("localStorage", [])
                            if origin and ls_items:
                                # Construct JS to set localStorage
                                ls_script = "\n".join([
                                    f"localStorage.setItem({json.dumps(item['name'])}, {json.dumps(item['value'])});"
                                    for item in ls_items
                                ])
                                # Use add_init_script to ensure it runs before any page scripts
                                await context.add_init_script(f"(function() {{ if (window.location.origin === '{origin}') {{ {ls_script} }} }})()")
                        logger.info(f"  -> Registered LocalStorage injection for {len(state_storage['origins'])} origins")
                            
            except Exception as e:
                logger.warning(f"  -> Failed to load state from {STATE_FILE}: {e}")

        logger.info(f"Starting crawl with {len(work_queue)} items in queue...")

        while work_queue:
            if consecutive_failures >= 5:
                logger.error("Too many consecutive failures. Stopping.")
                break

            url, is_index_hint = work_queue.pop(0)
            
            if url in processed_urls:
                continue
            
            # --- SCHEDULING LOGIC (Human Learning Pattern) ---
            now = datetime.datetime.now()
            current_hour = now.hour
            
            # 1. æ¯æ—¥/æ¯å°æ—¶é™é¢æ£€æŸ¥
            today_str = now.strftime("%Y-%m-%d")
            this_hour_str = now.strftime("%Y-%m-%d %H")
            
            # ä½¿ç”¨ random ç§å­ï¼ˆåŸºäºæ—¥æœŸï¼‰ç”Ÿæˆä»Šå¤©çš„å›ºå®šé™é¢ï¼Œå¢åŠ æ‹Ÿäººæ„Ÿ
            random.seed(today_str)
            daily_limit_today = random.randint(DAILY_LIMIT_MIN, DAILY_LIMIT_MAX)
            random.seed() # é‡ç½®ç§å­
            
            daily_data = {}
            if os.path.exists(DAILY_COUNT_FILE):
                try:
                    with open(DAILY_COUNT_FILE, "r") as f: daily_data = json.load(f)
                except: pass
            today_count = daily_data.get(today_str, 0)
            
            hourly_data = {}
            if os.path.exists(HOURLY_COUNT_FILE):
                try:
                    with open(HOURLY_COUNT_FILE, "r") as f: hourly_data = json.load(f)
                except: pass
            hour_count = hourly_data.get(this_hour_str, 0)
            
            # æ£€æŸ¥æ¯æ—¥ä¸Šé™
            if today_count >= daily_limit_today:
                logger.warning(f"ğŸ“Š [Limit] ä»Šæ—¥å·²è®¿é—® {today_count} ç¯‡ï¼Œè¾¾åˆ°ä»Šæ—¥åŠ¨æ€ä¸Šé™ {daily_limit_today}ã€‚")
                logger.warning(f"ğŸ“Š ç­‰å¾…è‡³æ˜å¤© 07:00 é‡ç½®...")
                tomorrow_7am = (now + datetime.timedelta(days=1)).replace(hour=7, minute=0, second=0)
                wait_seconds = (tomorrow_7am - now).total_seconds()
                await asyncio.sleep(wait_seconds)
                continue

            # æ£€æŸ¥æ¯å°æ—¶ä¸Šé™
            if hour_count >= HOURLY_LIMIT:
                logger.warning(f"â³ [Hourly Limit] æœ¬å°æ—¶å·²è®¿é—® {hour_count} ç¯‡ï¼Œè¾¾åˆ°ä¸Šé™ {HOURLY_LIMIT}ã€‚")
                next_hour = (now + datetime.timedelta(hours=1)).replace(minute=1, second=0)
                wait_seconds = (next_hour - now).total_seconds()
                logger.warning(f"â³ å°†åœ¨ä¸‹ä¸ªæ•´ç‚¹ ({next_hour.strftime('%H:%M')}) æ¢å¤ï¼Œç­‰å¾… {int(wait_seconds/60)} åˆ†é’Ÿ...")
                await asyncio.sleep(wait_seconds)
                continue
            
            # 2. å¤œé—´ä¼‘æ¯ (23:00 - 07:00)
            if current_hour >= NIGHT_START or current_hour < NIGHT_END:
                wake_time = now.replace(hour=NIGHT_END, minute=0, second=0)
                if current_hour >= NIGHT_START:
                    wake_time += datetime.timedelta(days=1)
                wait_seconds = (wake_time - now).total_seconds()
                logger.warning(f"ğŸŒ™ [ç¡çœ ] ç°åœ¨æ˜¯ {now.strftime('%H:%M')}ï¼Œè¿›å…¥å¤œé—´ä¼‘æ¯æ¨¡å¼ã€‚")
                logger.warning(f"ğŸŒ™ å°†äºæ˜æ—© {NIGHT_END}:00 è‡ªåŠ¨æ¢å¤ï¼Œç­‰å¾… {wait_seconds/3600:.1f} å°æ—¶...")
                save_progress(list(processed_urls), work_queue)
                await asyncio.sleep(wait_seconds)
                logger.info("â˜€ï¸ æ—©å®‰ï¼å¼€å§‹æ–°çš„ä¸€å¤©ã€‚")
                continue
            
            # 3. æ—¶æ®µæ„ŸçŸ¥åŠ¨æ€é—´éš”
            min_sleep, max_sleep = 30, 90  # é»˜è®¤å€¼
            current_slot = "default"
            for slot_name, slot_config in TIME_SLOTS.items():
                if current_hour in slot_config["hours"]:
                    min_sleep = slot_config["min_sleep"]
                    max_sleep = slot_config["max_sleep"]
                    current_slot = slot_name
                    break
            
            # 4. åˆä¼‘æ—¶æ®µç‰¹æ®Šå¤„ç†ï¼ˆ12:00-14:00 åŸºæœ¬æš‚åœï¼‰
            if current_slot == "lunch":
                logger.info(f"ğŸ½ï¸ [åˆä¼‘] ç°åœ¨æ˜¯åˆé¤æ—¶é—´ ({now.strftime('%H:%M')})ï¼Œæ”¾æ…¢èŠ‚å¥...")
            
            # 5. æ¯30åˆ†é’Ÿä¼‘æ¯5åˆ†é’Ÿ
            if time.time() - last_short_break > 1800:
                logger.info(f"â˜• [ä¼‘æ¯] è¿ç»­å·¥ä½œ30åˆ†é’Ÿï¼Œä¼‘æ¯5åˆ†é’Ÿ...")
                await asyncio.sleep(300)
                last_short_break = time.time()
                logger.info("â˜• ä¼‘æ¯ç»“æŸï¼Œç»§ç»­å­¦ä¹ ã€‚")
            # ---------------------------------------

            logger.info(f"[{len(processed_urls)+1} / Q:{len(work_queue)}] Processing: {url}")
            
            try:
                # 1. Navigation
                try:
                    await page.goto(url, wait_until="domcontentloaded", timeout=60000)
                except Exception as e:
                    logger.warning(f"  -> Nav warning: {e}")

                # 2. CSS Cleanup & Table Fixes
                await page.add_style_tag(content="""
                    /* 1. å½»åº•æš´åŠ›æ¸…ç†æ‰€æœ‰å¹²æ‰°å…ƒç´ ï¼ˆåªéšè—æŒ‰é’®å’Œé®ç½©ï¼Œä¸éšè—å†…å®¹ï¼‰ */
                    .header, .header-container, .top-nav, .nav-bar, .breadcrumb,
                    .footer, .sidebar, .thread-catelog, .el-dialog__wrapper, 
                    .v-note-op, .article-footer-operate, .thread-status, 
                    .myprofile-bomb-box, .el-backtop, .v-modal, .mask,
                    [class*="skeleton"], [class*="loading"], [class*="mask"],
                    [class*="overlay"], [class*="placeholder"], [class*="lazy"],
                    [class*="toolbar"], [class*="action-bar"], .copy-code-btn,
                    /* éšè—æŠ˜å /å±•å¼€æŒ‰é’®æœ¬èº«ï¼Œä½†ä¸éšè—å…¶åŒ…è£¹çš„å†…å®¹ */
                    [class*="expand"] i, [class*="collapse"] i, [class*="fold"] i,
                    .show-more, .read-more-btn { 
                        display: none !important; 
                        opacity: 0 !important;
                        visibility: hidden !important;
                    }

                    /* å¼ºåˆ¶æ˜¾ç¤ºå¯èƒ½è¢«æŠ˜å çš„å†…å®¹ */
                    [class*="content-hidden"], [class*="collapsed"], .is-collapsed {
                        display: block !important;
                        max-height: none !important;
                        visibility: visible !important;
                        opacity: 1 !important;
                    }

                    /* 2. æ·±åº¦é‡ç½®å¸ƒå±€æµï¼šå¼ºåˆ¶æ‰€æœ‰å…ƒç´ å›å½’æ ‡å‡†æ–‡æ¡£æµ */
                    * {
                        position: static !important;
                        float: none !important;
                        clear: none !important; /* åé¢ä¼šé’ˆå¯¹æ€§è®¾ç½® */
                        box-sizing: border-box !important;
                    }

                    html, body, #__nuxt, #__layout, .global, .w-100 {
                        display: block !important;
                        height: auto !important;
                        width: 100% !important;
                        overflow: visible !important;
                        margin: 0 !important;
                        padding: 0 !important;
                        background: white !important;
                    }

                    /* 3. æ–‡ç« å®¹å™¨ï¼šç¡®ä¿å®ƒæ˜¯å¸ƒå±€çš„ç¨³å›ºåŸºåº§ */
                    .article-cont { 
                        display: block !important;
                        width: 100% !important; 
                        padding: 20px !important; 
                        background: white !important;
                    }

                    /* å¼ºåˆ¶æ–‡ç« å†…éƒ¨çš„ç›´æ¥å­å…ƒç´ ï¼ˆå¦‚ h1, h2, p, pre, divï¼‰å‚ç›´çº¿æ€§æ’åˆ— */
                    .article-cont > *, .vditor-reset > * {
                        display: block !important;
                        clear: both !important; /* å¼ºåˆ¶æ¢è¡Œï¼Œé˜²æ­¢é‡å  */
                        margin-bottom: 1.2em !important;
                        position: static !important;
                        visibility: visible !important;
                        opacity: 1 !important;
                    }

                    /* 4. ä»£ç å—ï¼šé’ˆå¯¹æ€§ä¿®å¤é«˜åº¦è®¡ç®—é—®é¢˜ */
                    pre, code, .hljs, .vditor-reset pre, .vditor-reset code {
                        display: block !important;
                        width: 100% !important;
                        height: auto !important;
                        min-height: 1.5em !important;
                        max-height: none !important;
                        overflow: visible !important; /* ç¡®ä¿å†…å®¹æ’‘å¼€å®¹å™¨é«˜åº¦ */
                        white-space: pre-wrap !important; 
                        word-wrap: break-word !important;
                        word-break: break-all !important;
                        font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, Courier, monospace !important;
                        font-size: 11px !important;
                        line-height: 1.4 !important;
                        background: #f8f8f8 !important;
                        border: 1px solid #ddd !important;
                        padding: 12px !important;
                        margin: 20px 0 !important;
                        tab-size: 4 !important;
                    }

                    /* 5. è¯„è®ºåŒºï¼šæé™å‹ç¼©ä¸”ä¿æŒæ•´é½ */
                    .comment-list, .reply-list {
                        display: block !important;
                        margin-top: 30px !important;
                        border-top: 1px solid #eee !important;
                    }
                    .comment-item, .reply-item {
                        display: block !important;
                        padding: 8px 0 !important;
                        border-bottom: 1px solid #f0f0f0 !important;
                        clear: both !important;
                    }
                    .comment-item-header, .reply-item-header {
                        display: flex !important;
                        align-items: center !important;
                        margin-bottom: 4px !important;
                    }
                    .avatar {
                        width: 18px !important;
                        height: 18px !important;
                        margin-right: 8px !important;
                        border-radius: 50% !important;
                    }
                    .nickname { font-size: 11px !important; font-weight: bold !important; color: #555 !important; }
                    .time { font-size: 10px !important; color: #999 !important; margin-left: 10px !important; }
                    .comment-item-content, .reply-item-content {
                        font-size: 12px !important;
                        color: #333 !important;
                        padding-left: 26px !important;
                        line-height: 1.5 !important;
                    }

                    /* 6. ç‰¹æ®Šï¼šå½»åº•ç§»é™¤æ‰€æœ‰ä¼ªå…ƒç´ è£…é¥°ï¼Œé˜²æ­¢è«åå…¶å¦™çš„ç°è‰²çº¿æ¡/å— */
                    *::before, *::after {
                        display: none !important;
                        content: none !important;
                    }

                    /* 7. æ‰“å°ä¼˜åŒ– */
                    @media print {
                        * { -webkit-print-color-adjust: exact !important; }
                    }
                """)

                
                # 3. Enhanced Wait Strategy (Hydration)
                logger.info("  -> Waiting for hydration...")
                await asyncio.sleep(10) # Base wait
                
                # 4. å¾ªç¯æ»šåŠ¨è§¦åº•ï¼Œç¡®ä¿è§¦å‘æ‰€æœ‰æ‡’åŠ è½½ (Crucial for Long PDF)
                logger.info("  -> Scrolling to trigger lazy loads...")
                await page.evaluate("""async () => {
                    let lastHeight = document.documentElement.scrollHeight;
                    while (true) {
                        window.scrollBy(0, 1500);
                        await new Promise(r => setTimeout(r, 800));
                        let newHeight = document.documentElement.scrollHeight;
                        if (newHeight === lastHeight) {
                            // å†æ¬¡å°è¯•æ»šåŠ¨ä¸€æ®µè·ç¦»ï¼Œç¡®è®¤æ˜¯å¦çœŸçš„åˆ°åº•
                            window.scrollBy(0, 1000);
                            await new Promise(r => setTimeout(r, 1200));
                            if (document.documentElement.scrollHeight === newHeight) break;
                        }
                        lastHeight = newHeight;
                        if (lastHeight > 50000) break; // å®‰å…¨é˜ˆå€¼ï¼Œé˜²æ­¢æ— é™æ»šåŠ¨
                    }
                }""")
                await asyncio.sleep(2)
                
                # 5. å›åˆ°é¡¶éƒ¨æå–æ ‡é¢˜
                await page.evaluate("window.scrollTo(0, 0)")
                title = await get_clean_title(page)
                
                # --- AUTO-STOP PROTECTION (æ”¹è¿›ç‰ˆ) ---
                # åªæœ‰å½“æ ‡é¢˜å’Œå†…å®¹éƒ½ä¸æ­£å¸¸æ—¶æ‰åˆ¤å®šä¸º antibot
                is_title_generic = "ä¸»é¢˜è¯¦æƒ…é¡µ" in title or title == ""
                
                if is_title_generic:
                    # å…ˆæ£€æŸ¥é¡µé¢å†…å®¹æ˜¯å¦å·²ç»åŠ è½½ï¼ˆé€šè¿‡æ–‡ç« åŒºåŸŸæ˜¯å¦æœ‰å®è´¨å†…å®¹ï¼‰
                    content_check = await page.evaluate("""() => {
                        const article = document.querySelector('.article-cont') || 
                                        document.querySelector('.vditor-reset') || 
                                        document.querySelector('.thread-cont');
                        if (article && article.innerText.trim().length > 100) {
                            return { hasContent: true, length: article.innerText.length };
                        }
                        return { hasContent: false, length: 0 };
                    }""")
                    
                    if content_check.get("hasContent"):
                        # å†…å®¹å·²åŠ è½½ï¼Œåªæ˜¯æ ‡é¢˜è·å–å¤±è´¥ï¼Œä½¿ç”¨ thread_id ä½œä¸ºæ ‡é¢˜
                        logger.info(f"  -> æ ‡é¢˜è·å–å¤±è´¥ï¼Œä½†å†…å®¹å·²åŠ è½½ ({content_check.get('length')} chars)ï¼Œç»§ç»­å¤„ç†...")
                        title = f"Thread_{url.split('/')[-1].split('?')[0]}"
                    else:
                        # æ ‡é¢˜å’Œå†…å®¹éƒ½æ²¡æœ‰ï¼ŒçœŸæ­£çš„ antibot
                        debug_screenshot_path = f"error_antibot_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
                        await page.screenshot(path=debug_screenshot_path, full_page=True)
                        logger.warning(f"âš ï¸ [Anti-bot] Detected (Title='{title}'). Screenshot saved: {debug_screenshot_path}")
                        logger.warning(f"âš ï¸ [Anti-bot] Waiting 10 minutes before retrying...")
                        
                        # å‘é€ä¼ä¸šå¾®ä¿¡å‘Šè­¦ï¼ˆä½¿ç”¨é”™è¯¯ä¸“ç”¨ Webhookï¼‰
                        send_wecom_alert(
                            "è®ºå›çˆ¬è™« Anti-bot å‘Šè­¦",
                            f"> **æ£€æµ‹æ—¶é—´**: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
                            f"> **é—®é¢˜URL**: {url}\n"
                            f"> **å½“å‰æ ‡é¢˜**: {title}\n"
                            f"> **é˜Ÿåˆ—å‰©ä½™**: {len(work_queue)} ç¯‡\n\n"
                            f"ç¨‹åºå°†ç­‰å¾… 10 åˆ†é’Ÿåè‡ªåŠ¨é‡è¯•...",
                            is_error=True
                        )
                        
                        # Re-queue current item to ensure it gets retry later
                        work_queue.insert(0, (url, is_index_hint))
                        save_progress(list(processed_urls), work_queue)
                        await asyncio.sleep(600)  # Wait 10 minutes
                        continue  # Then continue with queue
                # -------------------------------------------

                safe_title = re.sub(r'[\\/*?:"<>|]', "", title).strip()
                thread_id = url.split("/")[-1].split("?")[0]
                if not safe_title:
                    safe_title = f"Topic_{thread_id}"
                
                pdf_filename = f"{safe_title}_{thread_id}.pdf"
                filepath = os.path.join(OUTPUT_DIR, pdf_filename)
                
                # 6. Content Check (to avoid blank PDFs & skip VIP posts)
                check_result = await page.evaluate("""() => {
                    const article = document.querySelector('.article-cont') || document.querySelector('.vditor-reset') || document.querySelector('.thread-cont');
                    const text = article ? article.innerText : "";
                    
                    // æ”¹è¿›åçš„ç™»å½•æ£€æµ‹ï¼šæ£€æŸ¥â€œé€€å‡ºâ€æ–‡å­—
                    const bodyText = document.body.innerText;
                    const is_logged_in = bodyText.includes('é€€å‡º') || 
                                         bodyText.includes('ä¸ªäººä¸­å¿ƒ') || 
                                         !!document.querySelector('.avatar');
                    
                    if (text.includes("å‰©ä½™å†…å®¹å·²éšè—") || text.includes("æŠ¥åè¯¾ç¨‹å³å¯æŸ¥çœ‹å®Œæ•´å†…å®¹")) {
                        return { status: "HIDDEN", is_logged_in: is_logged_in };
                    }
                    if (!article || article.innerText.trim().length < 50) {
                        return { status: "EMPTY", is_logged_in: is_logged_in };
                    }
                    return { status: "OK", is_logged_in: is_logged_in };
                }""")
                
                content_status = check_result.get("status")
                is_logged_in = check_result.get("is_logged_in", False)
                
                if content_status == "HIDDEN":
                    if not is_logged_in:
                        # æ ¸å¿ƒæ”¹åŠ¨ï¼šå¦‚æœæœªç™»å½•çœ‹åˆ°éšè—ï¼Œè®¤ä¸ºæ˜¯ä¼šè¯å¤±æ•ˆï¼Œè€Œä¸æ˜¯çœŸçš„VIP
                        logger.error(f"  -> [CRITICAL] ä¼šè¯å¤±æ•ˆï¼æ£€æµ‹åˆ°æœªç™»å½•ä¸”å†…å®¹è¢«éšè—ã€‚")
                        
                        # ä¿å­˜æ•…éšœå¿«ç…§
                        debug_dir = "temp_screenshots"
                        if not os.path.exists(debug_dir): os.makedirs(debug_dir)
                        fail_screenshot = os.path.join(debug_dir, f"session_fail_{datetime.datetime.now().strftime('%H%M%S')}.png")
                        await page.screenshot(path=fail_screenshot, full_page=True)
                        logger.info(f"  -> å·²ä¿å­˜ä¼šè¯å¤±æ•ˆæˆªå›¾: {fail_screenshot}")

                        send_wecom_alert(
                            "ğŸš¨ çˆ¬è™«ä¼šè¯å¤±æ•ˆ",
                            f"> **æ£€æµ‹æ—¶é—´**: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
                            f"> **URL**: {url}\n"
                            f"> **æ˜¾ç¤ºæ–‡æœ¬**: {check_result.get('status')}\n"
                            f"> **çŠ¶æ€**: æ£€æµ‹åˆ°æœªç™»å½•ï¼Œè¯·é‡æ–°è¿è¡Œ verify_and_refresh.py\n"
                            f"> **æˆªå›¾**: {fail_screenshot}",
                            is_error=True
                        )
                        # å°†å½“å‰ä»»åŠ¡æ”¾å›é˜Ÿåˆ—å¹¶åœæ­¢
                        work_queue.insert(0, (url, is_index_hint))
                        save_progress(list(processed_urls), work_queue)
                        break # ä¸­æ–­å¾ªç¯ï¼Œç­‰å¾…ç”¨æˆ·å¹²é¢„
                    
                    logger.warning(f"  -> [SKIP] VIP/æƒé™å¸–ï¼Œè·³è¿‡æ­¤å¸–ã€‚")
                    processed_urls.add(url)
                    save_progress(list(processed_urls), work_queue)
                    # å‘é€ skip é€šçŸ¥
                    send_wecom_alert(
                        "âš ï¸ è·³è¿‡ VIP/æƒé™å¸–",
                        f"> **æ—¶é—´**: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
                        f"> **URL**: {url}\n"
                        f"> è¿›åº¦: {len(processed_urls)}/{len(processed_urls)+len(work_queue)}",
                        is_error=True
                    )
                    await random_sleep(5, 10)
                    continue
                elif content_status != "OK":
                    logger.warning("  -> Content seems empty. Skipping.")
                    # å‘é€å†…å®¹ä¸ºç©ºé€šçŸ¥
                    send_wecom_alert(
                        "âš ï¸ å†…å®¹ä¸ºç©º",
                        f"> **æ—¶é—´**: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
                        f"> **URL**: {url}\n"
                        f"> å·²é‡æ–°åŠ å…¥é˜Ÿåˆ—ç­‰å¾…é‡è¯•",
                        is_error=True
                    )
                    consecutive_failures += 1
                    work_queue.append((url, is_index_hint))
                    await asyncio.sleep(30)
                    continue

                # 7. æ¨¡æ‹Ÿé˜…è¯»æ—¶é—´ï¼ˆæ ¹æ®å†…å®¹é•¿åº¦è®¡ç®—ï¼Œæ›´åŠ éšè”½ï¼‰
                content_length = await page.evaluate("""() => {
                    const article = document.querySelector('.article-cont') || 
                                    document.querySelector('.vditor-reset') || 
                                    document.querySelector('.thread-cont');
                    return article ? article.innerText.length : 500;
                }""")
                # å‡è®¾é˜…è¯»é€Ÿåº¦ 300-500 å­—/åˆ†é’Ÿï¼Œè®¡ç®—é˜…è¯»æ—¶é—´ï¼ˆç§’ï¼‰
                # æœ€å°‘15ç§’ï¼Œæœ€å¤š120ç§’
                base_read_time = max(15, min(120, content_length / 400 * 60))
                read_time = int(base_read_time * random.uniform(0.8, 1.3))  # åŠ å…¥éšæœºæ³¢åŠ¨
                logger.info(f"  -> æ¨¡æ‹Ÿé˜…è¯» {read_time} ç§’ (å†…å®¹ {content_length} å­—)...")
                
                # é˜…è¯»æ—¶æ¨¡æ‹Ÿç¼“æ…¢æ»šåŠ¨
                scroll_steps = random.randint(3, 6)
                for _ in range(scroll_steps):
                    await asyncio.sleep(read_time / scroll_steps)
                    scroll_amount = random.randint(200, 600)
                    await page.evaluate(f"window.scrollBy(0, {scroll_amount})")
                
                # 8. é˜…è¯»å®Œæ¯•ï¼Œå›åˆ°é¡¶éƒ¨å‡†å¤‡æ‰“å° PDF
                await page.evaluate("window.scrollTo(0, 0)")
                await asyncio.sleep(1)
                
                # 9. è·å–é¡µé¢æ€»é«˜åº¦ä»¥å®ç°â€œæ— åˆ†é¡µâ€é•¿å›¾ PDF
                height = await page.evaluate("() => document.documentElement.scrollHeight")
                # å¢åŠ ä¸€ç‚¹ç¼“å†²é«˜åº¦
                pdf_height = height + 50
                
                # 10. Print to PDF (Long Page, No Pagination)
                logger.info(f"  -> Printing Long PDF ({height}px): {pdf_filename}")
                await page.pdf(
                    path=filepath,
                    width="1200px",  # å›ºå®šå®½åº¦ï¼Œæ¨¡æ‹Ÿç½‘é¡µ
                    height=f"{pdf_height}px",
                    print_background=True,
                    margin={"top": "0px", "bottom": "0px", "left": "0px", "right": "0px"}
                )
                
                logger.info(f"  -> Success!")
                processed_urls.add(url)
                consecutive_failures = 0
                
                # å‘é€æˆåŠŸé€šçŸ¥åˆ°ä¼ä¸šå¾®ä¿¡ï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰
                send_wecom_alert(
                    "âœ… çˆ¬å–æˆåŠŸ",
                    f"> **{safe_title}**\n"
                    f"> {url}\n"
                    f"> æ—¶é—´: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
                    f"> è¿›åº¦: {len(processed_urls)}/{len(processed_urls)+len(work_queue)} | å‰©ä½™: {len(work_queue)}"
                )

                # 10. Save Progress (EVERY TIME for safety)
                save_progress(list(processed_urls), work_queue)
                
                # 11. æ›´æ–°è®¡æ•°å™¨
                daily_data[today_str] = daily_data.get(today_str, 0) + 1
                with open(DAILY_COUNT_FILE, "w") as f:
                    json.dump(daily_data, f)
                
                hourly_data[this_hour_str] = hourly_data.get(this_hour_str, 0) + 1
                with open(HOURLY_COUNT_FILE, "w") as f:
                    json.dump(hourly_data, f)
                
                # 10. æ—¶æ®µæ„ŸçŸ¥åŠ¨æ€é—´éš”
                sleep_time = random.randint(min_sleep, max_sleep)
                logger.info(f"  -> [Safety] æ—¶æ®µ:{current_slot} ä¼‘æ¯ {sleep_time} ç§’...")
                await asyncio.sleep(sleep_time)

            except Exception as e:
                logger.error(f"Error processing {url}: {e}")
                consecutive_failures += 1
                work_queue.append((url, is_index_hint))
                await asyncio.sleep(30)

        await context.close()
    
    save_progress(list(processed_urls), work_queue)
    logger.info("ğŸ‰ é˜Ÿåˆ—å·²æ¸…ç©ºæˆ–è¾¾åˆ°ä¸Šé™ï¼Œçˆ¬è™«ä»»åŠ¡å®Œæˆã€‚")

if __name__ == "__main__":
    asyncio.run(process_content())
