"""
é‚¢ä¸è¡Œï½œç­–ç•¥åˆ†äº«ä¼š
ä»“ä½ç®¡ç†æ¡†æ¶

ç‰ˆæƒæ‰€æœ‰ Â©ï¸ é‚¢ä¸è¡Œ
å¾®ä¿¡: xbx1717

æœ¬ä»£ç ä»…ä¾›ä¸ªäººå­¦ä¹ ä½¿ç”¨ï¼Œæœªç»æˆæƒä¸å¾—å¤åˆ¶ã€ä¿®æ”¹æˆ–ç”¨äºå•†ä¸šç”¨é€”ã€‚

Author: é‚¢ä¸è¡Œ
"""
import gc
import os
import time
import warnings
from concurrent.futures import ProcessPoolExecutor, as_completed
from pathlib import Path
from typing import List

import numpy as np
import pandas as pd
import polars as pl
from tqdm import tqdm

from config import job_num, factor_col_limit
from core.factor import calc_factor_vals
from core.model.backtest_config import BacktestConfig, StrategyConfig
from core.utils.factor_hub import FactorHub
from core.utils.log_kit import logger
from core.utils.path_kit import get_file_path

warnings.filterwarnings('ignore')
# pandasç›¸å…³çš„æ˜¾ç¤ºè®¾ç½®ï¼ŒåŸºç¡€è¯¾ç¨‹éƒ½æœ‰ä»‹ç»
pd.set_option('display.max_rows', 1000)
pd.set_option('expand_frame_repr', False)  # å½“åˆ—å¤ªå¤šæ—¶ä¸æ¢è¡Œ
pd.set_option('display.unicode.ambiguous_as_wide', True)  # è®¾ç½®å‘½ä»¤è¡Œè¾“å‡ºæ—¶çš„åˆ—å¯¹é½åŠŸèƒ½
pd.set_option('display.unicode.east_asian_width', True)

# è®¡ç®—å®Œå› å­ä¹‹åï¼Œä¿ç•™çš„å­—æ®µ
KLINE_COLS = ['candle_begin_time', 'symbol', 'is_spot', 'close', 'next_close', 'symbol_spot', 'symbol_swap', 'æ˜¯å¦äº¤æ˜“']
# è®¡ç®—å®Œé€‰å¸ä¹‹åï¼Œä¿ç•™çš„å­—æ®µ
SELECT_RES_COLS = [*KLINE_COLS, 'strategy', 'cap_weight', 'æ–¹å‘', 'offset', 'target_alloc_ratio', 'order_first']
# å®Œæ•´klineæ•°æ®ä¿å­˜çš„è·¯å¾„
ALL_KLINE_PATH_TUPLE = ('data', 'cache', 'all_factors_kline.pkl')
ALL_KLINE_FULL_PATH_TUPLE = ('data', 'cache', 'all_factors_kline_full.pkl')


# ======================================================================================
# å› å­è®¡ç®—ç›¸å…³å‡½æ•°
# - calc_factors_by_symbol: è®¡ç®—å•ä¸ªå¸ç§çš„å› å­æ± 
# - calc_factors: è®¡ç®—å› å­æ± 
# ======================================================================================

def trans_period_for_day(df, date_col='candle_begin_time', factor_dict=None):
    """
    å°†æ•°æ®å‘¨æœŸè½¬æ¢ä¸ºæŒ‡å®šçš„1Då‘¨æœŸ
    :param df: åŸå§‹æ•°æ®
    :param date_col: æ—¥æœŸåˆ—
    :param factor_dict: è½¬æ¢è§„åˆ™
    :return:
    """
    df.set_index(date_col, inplace=True)
    # å¿…å¤‡å­—æ®µ
    agg_dict = {
        'symbol': 'first',
        'open': 'first',
        'high': 'max',
        'low': 'min',
        'close': 'last',
        'volume': 'sum',
        'quote_volume': 'sum',
        'trade_num': 'sum',
        'taker_buy_base_asset_volume': 'sum',
        'taker_buy_quote_asset_volume': 'sum',
        'is_spot': 'last',
        # 'has_swap': 'last',
        'symbol_swap': 'last',
        'symbol_spot': 'last',
        'funding_fee': 'sum',
        'next_avg_price': 'last',
        'æ˜¯å¦äº¤æ˜“': 'last',
    }

    if factor_dict:
        agg_dict = dict(agg_dict, **factor_dict)
    df = df.resample('1D').agg(agg_dict)
    df.reset_index(inplace=True)

    return df


from core.utils.factor_cache import load_factor_cache, save_factor_cache

# region å› å­è®¡ç®—ç›¸å…³å‡½æ•°
def calc_factors_by_candle(candle_df, conf: BacktestConfig, factor_col_name_list) -> pd.DataFrame:
    """
    é’ˆå¯¹å•ä¸€æ¯”å¯¹ï¼Œè®¡ç®—æ‰€æœ‰å› å­çš„æ•°å€¼
    :param candle_df: ä¸€ä¸ªå¸ç§çš„kçº¿æ•°æ® dataframe
    :param conf: å›æµ‹é…ç½®
    :param factor_col_name_list: éœ€è¦è®¡ç®—çš„å› å­åˆ—
    :return: åŒ…å«æ‰€æœ‰å› å­çš„ dataframe(ç›®å‰æ˜¯åŒ…å«kçº¿æ•°æ®çš„ï¼‰
    """
    # éå†æ¯ä¸ªå› å­ï¼Œè®¡ç®—æ¯ä¸ªå› å­çš„æ•°æ®
    factor_series_dict = {}
    symbol = candle_df['symbol'].iloc[0]
    first_candle = candle_df['candle_begin_time'].iloc[0]
    last_candle = candle_df['candle_begin_time'].iloc[-1]

    for factor_name, param_list in conf.factor_params_dict.items():
        factor = FactorHub.get_by_name(factor_name)  # è·å–å› å­ä¿¡æ¯
        if factor.is_cross:
            continue

        # ç­›é€‰ä¸€ä¸‹éœ€è¦è®¡ç®—çš„å› å­
        factor_param_list = []
        for param in param_list:
            factor_col_name = f'{factor_name}_{param}'
            if factor_col_name in factor_col_name_list:
                factor_param_list.append(param)
        if len(factor_param_list) == 0:
            continue  # å½“è¯¥å› å­ä¸éœ€è¦è®¡ç®—çš„æ—¶å€™ç›´æ¥è¿”å›

        # ==========================
        # å°è¯•ä»ç¼“å­˜è¯»å– (L1 ä¼˜åŒ–)
        # ==========================
        cached_df = load_factor_cache(symbol, factor_name, factor_param_list, first_candle, last_candle)
        if cached_df is not None:
            # è½¬æ¢ä¸º dict of series
            for col in cached_df.columns:
                factor_series_dict[col] = cached_df[col].values
        else:
            # ç¼“å­˜æœªå‘½ä¸­ï¼Œæ‰§è¡Œè®¡ç®—
            res_dict = calc_factor_vals(candle_df, factor_name, factor_param_list)
            factor_series_dict.update(res_dict)
            # ä¿å­˜åˆ°ç¼“å­˜
            save_factor_cache(pd.DataFrame(res_dict), symbol, factor_name, factor_param_list, first_candle, last_candle)

    # å°†ç»“æœ DataFrame ä¸åŸå§‹ DataFrame åˆå¹¶
    kline_with_factor_dict = {
        'candle_begin_time': candle_df['candle_begin_time'].values,
        'symbol': candle_df['symbol'].values,
        'is_spot': candle_df['is_spot'].values,
        'close': candle_df['close'].values,
        # 'has_swap': candle_df['has_swap'],
        # 'next_avg_price': candle_df['next_avg_price'].values,
        'next_close': candle_df['close'].shift(-1).values,  # åé¢å‘¨æœŸæ’é™¤éœ€è¦ç”¨
        # 'next_funding_fee': candle_df['funding_fee'].shift(-1).values,
        'symbol_spot': candle_df['symbol_spot'].astype(str).values,
        'symbol_swap': candle_df['symbol_swap'].astype(str).values,
        **factor_series_dict,
        'æ˜¯å¦äº¤æ˜“': candle_df['æ˜¯å¦äº¤æ˜“'].values,
    }

    kline_with_factor_df = pd.DataFrame(kline_with_factor_dict, copy=False)
    kline_with_factor_df.sort_values(by='candle_begin_time', inplace=True)

    # æŠ›å¼ƒä¸€å¼€å§‹çš„ä¸€æ®µkçº¿ï¼Œä¿ç•™åé¢çš„æ•°æ®
    first_candle_time = candle_df.iloc[0]['first_candle_time'] + pd.to_timedelta(f'{conf.min_kline_num}h')

    # # è°ƒæ•´ symbol_spot å’Œ symbol_swap
    # for col in ['symbol_spot', 'symbol_swap']:
    #     symbol_start_time = candle_df[
    #         (candle_df[col] != '') & (candle_df[col].shift(1) == '') & (~candle_df[col].shift(1).isna())
    #         ]['candle_begin_time']
    #     if not symbol_start_time.empty:
    #         condition = pd.Series(False, index=kline_with_factor_df.index)
    #         for symbol_time in symbol_start_time:
    #             _cond1 = kline_with_factor_df['candle_begin_time'] >= symbol_time
    #             _cond2 = kline_with_factor_df['candle_begin_time'] <= symbol_time + pd.to_timedelta(
    #                 f'{conf.min_kline_num}h')
    #             condition |= (_cond1 & _cond2)
    #         kline_with_factor_df.loc[condition, col] = ''
    #     kline_with_factor_df[col] = kline_with_factor_df[col].astype('category')

    # éœ€è¦å¯¹æ•°æ®è¿›è¡Œè£åˆ‡
    kline_with_factor_df = kline_with_factor_df[kline_with_factor_df['candle_begin_time'] >= first_candle_time]

    # ä¸‹æ¶å¸/æ‹†åˆ†å¸ï¼Œå»æ‰æœ€åä¸€ä¸ªå‘¨æœŸä¸å…¨çš„æ•°æ®
    if kline_with_factor_df['candle_begin_time'].max() < pd.to_datetime(conf.end_date):
        _temp_time = kline_with_factor_df['candle_begin_time'] + pd.Timedelta(conf.max_hold_period)
        _del_time = kline_with_factor_df[kline_with_factor_df.loc[_temp_time.index, 'next_close'].isna()][
            'candle_begin_time']
        kline_with_factor_df = kline_with_factor_df[
            kline_with_factor_df['candle_begin_time'] <= _del_time.min() - pd.Timedelta(conf.max_hold_period)]

    # åªä¿ç•™æœ€è¿‘çš„æ•°æ®
    if not conf.has_section_factor:
        kline_with_factor_df = kline_with_factor_df[
            (kline_with_factor_df['candle_begin_time'] >= pd.to_datetime(conf.start_date)) &
            (kline_with_factor_df['candle_begin_time'] < pd.to_datetime(conf.end_date))]

    # åªä¿ç•™éœ€è¦çš„å­—æ®µ
    return kline_with_factor_df


def process_candle_df(candle_df: pd.DataFrame, conf: BacktestConfig, factor_col_name_list: List[str], idx: int):
    """
    # é’ˆå¯¹æ¯ä¸€ä¸ªå¸ç§çš„kçº¿æ•°æ®ï¼ŒæŒ‰ç…§ç­–ç•¥å¾ªç¯è®¡ç®—å› å­ä¿¡æ¯
    :param candle_df: å•ä¸ªå¸ç§çš„æ•°æ®
    :param conf: backtest config
    :param factor_col_name_list:    å› å­åˆ—è¡¨ï¼Œå¯ä»¥ç”¨äºåŠ¨æ€åˆ¤æ–­å½“å‰éœ€è¦è®¡ç®—çš„å› å­åˆ—ã€‚
                                    å½“ factor_col_name_list â‰  conf.factor_col_name_list æ—¶ï¼Œè¯´æ˜éœ€è¦èŠ‚çœä¸€ç‚¹å†…å­˜
    :param idx: ç´¢å¼•
    :return: å¸¦æœ‰å› å­æ•°å€¼çš„æ•°æ®
    """
    # ==== æ•°æ®é¢„å¤„ç† ====
    factor_dict = {'first_candle_time': 'first', 'last_candle_time': 'last'}
    for strategy in conf.strategy_list:
        symbol = candle_df['symbol'].iloc[-1]
        candle_df, _factor_dict, _ = strategy.after_merge_index(candle_df, symbol, factor_dict, {})
        factor_dict.update(_factor_dict)

    # è®¡ç®—å¹³å‡å¼€ç›˜ä»·æ ¼
    candle_df['next_avg_price'] = candle_df[conf.avg_price_col].shift(-1)  # ç”¨äºåé¢è®¡ç®—å½“å‘¨æœŸæ¶¨è·Œå¹…

    # è½¬æ¢æˆæ—¥çº¿æ•°æ®  è·Ÿå›æµ‹ä¿æŒä¸€è‡´
    if conf.is_day_period:
        candle_df = trans_period_for_day(candle_df, factor_dict=factor_dict)

    # ==== è®¡ç®—å› å­ ====
    # æ¸…ç†æ‰å¤´éƒ¨å‚ä¸æ—¥çº¿è½¬æ¢çš„å¡«å……æ•°æ®
    candle_df.dropna(subset=['symbol'], inplace=True)
    candle_df.reset_index(drop=True, inplace=True)
    # é’ˆå¯¹å•ä¸ªå¸ç§çš„Kçº¿æ•°æ®è®¡ç®—
    # è¿”å›å¸¦æœ‰å› å­æ•°å€¼çš„Kçº¿æ•°æ®
    factor_df = calc_factors_by_candle(candle_df, conf, factor_col_name_list)

    return idx, factor_df


def calc_factors(conf: BacktestConfig):
    """
    é€‰å¸å› å­è®¡ç®—ï¼Œè€ƒè™‘åˆ°å¤§å› å­å›æµ‹çš„åœºæ™¯ï¼Œæˆ‘ä»¬å¼•å…¥chunkçš„æ¦‚å¿µï¼Œä¼šæŠŠæ‰€æœ‰factoråˆ‡æˆå¤šåˆ†ï¼Œç„¶ååˆ†åˆ«è®¡ç®—
    :param conf:       è´¦æˆ·ä¿¡æ¯
    :return:
    """
    # ====================================================================================================
    # 1. ** kçº¿æ•°æ®æ•´ç†åŠå‚æ•°å‡†å¤‡ **
    # - is_use_spot: Trueçš„æ—¶å€™ï¼Œä½¿ç”¨ç°è´§æ•°æ®å’Œåˆçº¦æ•°æ®;
    # - Falseçš„æ—¶å€™ï¼Œåªä½¿ç”¨åˆçº¦æ•°æ®ã€‚æ‰€ä»¥è¿™ä¸ªæƒ…å†µæ›´ç®€å•
    # ====================================================================================================
    # hold_periodçš„ä½œç”¨æ˜¯è®¡ç®—å®Œå› å­ä¹‹åï¼Œ
    # è·å–æœ€è¿‘ hold_period ä¸ªå°æ—¶å†…çš„æ•°æ®ä¿¡æ¯ï¼Œ
    # åŒæ—¶ç”¨äºoffsetå­—æ®µè®¡ç®—ä½¿ç”¨
    # ====================================================================================================
    # 2. ** å› å­è®¡ç®— (V2 ä¼˜åŒ–ç‰ˆ) **
    # ====================================================================================================
    # ä¼˜å…ˆåŠ è½½ Parquet æ ¼å¼æ•°æ® (Zero-Copy å‡†å¤‡)
    candle_pq_path = get_file_path('data', 'cache', 'all_candle_data.parquet', as_path_type=True)
    if candle_pq_path.exists():
        logger.debug("âš¡ï¸ æ­£åœ¨é€šè¿‡ Polars åŠ è½½ Parquet åŸå§‹æ•°æ®...")
        full_df = pl.read_parquet(candle_pq_path)
        # è½¬æ¢ä¸º list of pandas (æš‚æ—¶ä¿æŒå› å­å‡½æ•°å…¼å®¹æ€§)
        candle_df_list = [group.to_pandas() for group in full_df.partition_by("symbol", maintain_order=True)]
        del full_df
    else:
        # å…œåº•ï¼šå¦‚æœ parquet ä¸å­˜åœ¨åˆ™å›é€€
        candle_df_list = pd.read_pickle(get_file_path('data', 'cache', 'all_candle_df_list.pkl'))
    
    factor_col_count = len(conf.factor_col_name_list)
    shards = range(0, factor_col_count, factor_col_limit)

    logger.debug(f'''* æ€»å…±è®¡ç®—å› å­ä¸ªæ•°ï¼š{factor_col_count} ä¸ª
* å•æ¬¡è®¡ç®—å› å­ä¸ªæ•°ï¼š{factor_col_limit} ä¸ªï¼Œ(éœ€åˆ†æˆ{len(shards)}ç»„è®¡ç®—)
* éœ€è¦è®¡ç®—å¸ç§æ•°é‡ï¼š{len(candle_df_list)} ä¸ª''')

    # æ¸…ç† cache çš„ç¼“å­˜
    all_kline_pkl = get_file_path(*ALL_KLINE_PATH_TUPLE, as_path_type=True)
    all_kline_pkl.unlink(missing_ok=True)

    all_kline_full_pkl = get_file_path(*ALL_KLINE_FULL_PATH_TUPLE, as_path_type=True)
    all_kline_full_pkl.unlink(missing_ok=True)

    for shard_index in shards:
        logger.info(f'å› å­åˆ†ç‰‡è®¡ç®—ä¸­ï¼Œè¿›åº¦ï¼š{int(shard_index / factor_col_limit) + 1}/{len(shards)}')
        factor_col_name_list = conf.factor_col_name_list[shard_index:shard_index + factor_col_limit]

        all_factor_df_list = []
        
        # V2 ä¼˜åŒ–ï¼šå¦‚æœç¼“å­˜å‘½ä¸­ç‡é«˜ï¼Œå¹¶è¡Œåè€Œæ›´æ…¢ (åºåˆ—åŒ–å¼€é”€ > è®¡ç®—å¼€é”€)
        # è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ ThreadPoolExecutor æ›¿ä»£ ProcessPoolExecutorï¼Œå› ä¸ºå¤§éƒ¨åˆ†æ“ä½œæ˜¯ I/O (è¯»ç¼“å­˜)
        # ä¸”é¿å…äº† DataFrames çš„åºåˆ—åŒ–å¼€é”€
        from concurrent.futures import ThreadPoolExecutor
        with ThreadPoolExecutor(max_workers=job_num) as executor:
            futures = [executor.submit(
                process_candle_df, candle_df, conf, factor_col_name_list, candle_idx
            ) for candle_idx, candle_df in enumerate(candle_df_list)]

            for future in tqdm(as_completed(futures), total=len(candle_df_list), desc='ğŸ§® æ—¶åºå› å­è®¡ç®—'):
                idx, factor_df = future.result()
                all_factor_df_list.append(factor_df)

        # ====================================================================================================
        # 3. ** åˆå¹¶å› å­ç»“æœ **
        # åˆå¹¶å¹¶æ•´ç†æ‰€æœ‰Kçº¿ï¼Œåˆ°è¿™é‡Œå› å­è®¡ç®—å®Œæˆ
        # ====================================================================================================
        all_factors_df = pd.concat(all_factor_df_list, ignore_index=True)
        all_factors_df['symbol'] = pd.Categorical(all_factors_df['symbol'])

        del all_factor_df_list

        # ====================================================================================================
        # 4. ** å› å­ç»“æœåˆ†ç‰‡å­˜å‚¨ **
        # åˆ†ç‰‡å­˜å‚¨è®¡ç®—ç»“æœï¼ŒèŠ‚çœå†…å­˜å ç”¨ï¼Œæé«˜é€‰å¸æ•ˆç‡
        # - å°†åˆå¹¶å¥½çš„dfï¼Œåˆ†æˆ2ä¸ªéƒ¨åˆ†ï¼škçº¿å’Œå› å­åˆ—
        # - kçº¿æ•°æ®å­˜å‚¨ä¸ºä¸€ä¸ªpklï¼Œæ¯ä¸€åˆ—å› å­å­˜å‚¨ä¸ºä¸€ä¸ªpklï¼Œåœ¨é€‰å¸æ—¶å€™æŒ‰éœ€è¯»å…¥åˆå¹¶æˆdf
        # ====================================================================================================
        logger.debug('ğŸ’¾ åˆ†ç‰‡å­˜å‚¨å› å­ç»“æœ...')

        # é€‰å¸éœ€è¦çš„kçº¿
        if not all_kline_pkl.exists():
            # å­˜å‚¨è£åˆ‡æ—¶é—´çš„æ•°æ®
            all_kline_df = all_factors_df[KLINE_COLS].sort_values(by=['candle_begin_time', 'symbol', 'is_spot'])
            all_kline_df = all_kline_df[
                (all_kline_df['candle_begin_time'] >= pd.to_datetime(conf.start_date)) &
                (all_kline_df['candle_begin_time'] < pd.to_datetime(conf.end_date))]
            all_kline_df.to_pickle(all_kline_pkl)
            # åŒæ—¶ä¿å­˜ Parquet (V2 ä¼˜åŒ–)
            all_kline_df.to_parquet(all_kline_pkl.with_suffix('.parquet'), index=False)

        if not all_kline_full_pkl.exists() and conf.has_section_factor:
            # å­˜å‚¨ä¸è£åˆ‡çš„å…¨é‡æ•°æ®
            all_kline_full_df = all_factors_df[KLINE_COLS].sort_values(by=['candle_begin_time', 'symbol', 'is_spot'])
            all_kline_full_df.to_pickle(all_kline_full_pkl)
            all_kline_full_df.to_parquet(all_kline_full_pkl.with_suffix('.parquet'), index=False)

        # é’ˆå¯¹æ¯ä¸€ä¸ªå› å­è¿›è¡Œå­˜å‚¨
        cut_factors_df = all_factors_df[
                (all_factors_df['candle_begin_time'] >= pd.to_datetime(conf.start_date)) &
                (all_factors_df['candle_begin_time'] < pd.to_datetime(conf.end_date))]
        # V2 ä¼˜åŒ–ï¼šå°†å› å­åˆ†ç‰‡å­˜å‚¨ä¸ºå•ä¸ª Parquet æ–‡ä»¶ï¼Œæå¤§å‡å°‘æ–‡ä»¶æ“ä½œå¼€é”€
        shard_pq = get_file_path('data', 'cache', f'factors_shard_{shard_index}.parquet', as_path_type=True)
        shard_pq.unlink(missing_ok=True)
        
        # ç¡®ä¿åˆ—éƒ½å­˜åœ¨
        valid_cols = [c for c in factor_col_name_list if c in all_factors_df.columns]
        save_cols = ['candle_begin_time', 'symbol', 'is_spot'] + valid_cols
        
        if conf.has_section_factor:
            shard_full_pq = get_file_path('data', 'cache', f'factors_full_shard_{shard_index}.parquet', as_path_type=True)
            shard_full_pq.unlink(missing_ok=True)
            all_factors_df[save_cols].to_parquet(shard_full_pq, index=False)
            
        cut_factors_df[save_cols].to_parquet(shard_pq, index=False)

        del all_factors_df, cut_factors_df

        gc.collect()


def process_factor_df(factor_col_name):
    # å‡†å¤‡æ‰€æœ‰æ—¶åºå› å­æ•°æ®
    factor_path = get_file_path('data', 'cache', f'factor_full_{factor_col_name}.pkl', as_path_type=True)
    if not factor_path.exists():
        return factor_col_name, pd.DataFrame()

    return factor_col_name, pd.read_pickle(factor_path)


def load_all_factors(conf: BacktestConfig):
    all_kline_full_pq = get_file_path(*ALL_KLINE_FULL_PATH_TUPLE, as_path_type=True).with_suffix('.parquet')
    if all_kline_full_pq.exists():
        factor_df = pd.read_parquet(all_kline_full_pq)
    else:
        factor_df = pd.read_pickle(get_file_path(*ALL_KLINE_FULL_PATH_TUPLE, as_path_type=True))

    # å‡†å¤‡æ‰€æœ‰æ—¶åºå› å­æ•°æ® (V2 ThreadPool ä¼˜åŒ–)
    from concurrent.futures import ThreadPoolExecutor
    with ThreadPoolExecutor(max_workers=job_num) as executor:
        futures = [executor.submit(
            process_factor_df, factor_col_name
        ) for factor_col_name in conf.section_depend_factor_col_name_list]

        for future in tqdm(as_completed(futures), total=len(conf.section_depend_factor_col_name_list), desc='âœ‚ï¸ è£åˆ‡æ—¶åºå› å­æ•°æ®'):
            factor_col_name, kline_with_factor_df = future.result()
            if not kline_with_factor_df.empty:
                factor_df[factor_col_name] = kline_with_factor_df

    return factor_df


def calc_cross_sections(conf: BacktestConfig):
    """
    æˆªé¢å› å­è®¡ç®—ï¼Œ
    :param conf:       è´¦æˆ·ä¿¡æ¯
    :return:
    """
    section_params_dict = conf.section_params_dict
    # å¦‚æœæ²¡æœ‰é…ç½®æˆªé¢å› å­ï¼Œé‚£ä¹ˆç›´æ¥è·³è¿‡åç»­
    if not section_params_dict:
        logger.info(f'æœªæ£€æŸ¥åˆ°æˆªé¢å› å­é…ç½®ï¼Œè·³è¿‡è®¡ç®—æˆªé¢å› å­æ­¥éª¤ã€‚')
        return

    # åŠ è½½é¢æ¿æ•°æ®
    factor_df = load_all_factors(conf)
    # condition = factor_df['is_spot'] == (1 if conf.is_use_spot else 0)
    # factor_df = factor_df.loc[condition, :]
    # factor_spot_df = factor_df.loc[factor_df['is_spot'] == 1, :].copy()
    # factor_swap_df = factor_df.loc[factor_df['is_spot'] == 0, :].copy()

    # éå†æˆªé¢å› å­ï¼Œè°ƒç”¨æˆªé¢å› å­è®¡ç®—æ–¹æ³•
    # factor_series_dict = {}
    for factor_name, param_list in section_params_dict.items():
        factor = FactorHub.get_by_name(factor_name)  # è·å–å› å­ä¿¡æ¯
        if not factor.is_cross:
            continue

        # ç­›é€‰ä¸€ä¸‹éœ€è¦è®¡ç®—çš„å› å­
        factor_param_list = []
        section_param_list = []
        for param in param_list:
            factor_col_name = f'{factor_name}_{param}'
            if factor_col_name in conf.factor_col_name_list:
                factor_param_list.append(param)
                section_param_list.extend(factor.get_factor_list(param))
        if len(factor_param_list) == 0:
            continue  # å½“è¯¥å› å­ä¸éœ€è¦è®¡ç®—çš„æ—¶å€™ç›´æ¥è¿”å›

        # æˆªé¢å› å­ä¾èµ–çš„æ—¶åºå› å­åˆ—
        section_col_name_list = list(set(f'{f}_{n}' for f, n in set(section_param_list)))

        # å¯¹æˆªé¢å› å­æŒ‰ç…§æ—¶é—´è¿›è¡Œåˆ†æ®µè®¡ç®—
        legacy_candle_df = factor_df[KLINE_COLS + section_col_name_list].copy()  # å¦‚æœæ˜¯è€çš„å› å­è®¡ç®—é€»è¾‘ï¼Œå•ç‹¬æ‹¿å‡ºæ¥ä¸€ä»½æ•°æ®
        for param in tqdm(factor_param_list, total=len(factor_param_list), desc=f'ğŸ§® æˆªé¢å› å­è®¡ç®—'):
            factor_col_name = f'{factor_name}_{param}'
            legacy_candle_df = factor.signal(legacy_candle_df, param, factor_col_name)

            # å¯¹æ•°æ®è¿›è¡Œè£åˆ‡å¹¶ä¿å­˜ (V2 ä¼˜åŒ–: ä¿æŒ Key å­—æ®µä¾¿äº Joinï¼Œæ”¹ç”¨ Parquet)
            cross_factor_df = legacy_candle_df[['candle_begin_time', 'symbol', 'is_spot', factor_col_name]]
            cross_factor_df = cross_factor_df[
                (cross_factor_df['candle_begin_time'] >= pd.to_datetime(conf.start_date)) &
                (cross_factor_df['candle_begin_time'] < pd.to_datetime(conf.end_date))]
            
            factor_pq = get_file_path('data', 'cache', f'factor_{factor_col_name}.parquet', as_path_type=True)
            factor_pq.unlink(missing_ok=True)
            cross_factor_df.to_parquet(factor_pq, index=False)
            del cross_factor_df
        del legacy_candle_df
    del factor_df
    gc.collect()


# endregion


# ======================================================================================
# é€‰å¸ç›¸å…³å‡½æ•°
# - calc_select_factor_rank: è®¡ç®—å› å­æ’åº
# - select_long_and_short_coin: é€‰åšå¤šå’Œåšç©ºçš„å¸ç§
# - select_coins_by_strategy: æ ¹æ®ç­–ç•¥é€‰å¸
# - select_coins: é€‰å¸ï¼Œå¾ªç¯ç­–ç•¥è°ƒç”¨ `select_coins_by_strategy`
# ======================================================================================
# region é€‰å¸ç›¸å…³å‡½æ•°
def calc_select_factor_rank(df, factor_column='å› å­', ascending=True):
    """
    è®¡ç®—å› å­æ’å (Polars ä¼˜åŒ–ç‰ˆæœ¬)
    :param df:              åŸæ•°æ®
    :param factor_column:   éœ€è¦è®¡ç®—æ’åçš„å› å­åç§°
    :param ascending:       è®¡ç®—æ’åé¡ºåºï¼ŒTrueï¼šä»å°åˆ°å¤§æ’åºï¼›Falseï¼šä»å¤§åˆ°å°æ’åº
    :return:                è®¡ç®—æ’ååçš„æ•°æ®æ¡†
    """
    # ä½¿ç”¨ Polars è¿›è¡Œé«˜æ€§èƒ½æ’åè®¡ç®—
    # Polars ä½¿ç”¨ Rust å¤šçº¿ç¨‹å¼•æ“ï¼Œæ¯” Pandas å¿« 3-10 å€
    
    # è½¬æ¢ä¸º Polars LazyFrame
    pl_df = pl.from_pandas(df).lazy()
    
    # è®¡ç®—åˆ†ç»„æ’åå’Œç›¸å…³ç»Ÿè®¡
    # descending å‚æ•°ä¸ Pandas ascending ç›¸å
    pl_result = pl_df.with_columns([
        pl.col(factor_column).rank(method='min', descending=not ascending).over('candle_begin_time').alias('rank'),
    ]).with_columns([
        pl.col('rank').max().over('candle_begin_time').alias('rank_max'),
        pl.col('symbol').count().over('candle_begin_time').alias('æ€»å¸æ•°'),
    ]).sort(['candle_begin_time', 'rank']).collect()
    
    # è½¬æ¢å› Pandas DataFrame
    result_df = pl_result.to_pandas()
    
    return result_df


def select_long_and_short_coin(strategy: StrategyConfig, long_df: pd.DataFrame, short_df: pd.DataFrame):
    """
    é€‰å¸ï¼Œæ·»åŠ å¤šç©ºèµ„é‡‘æƒé‡åï¼Œå¯¹äºæ— æƒé‡çš„æƒ…å†µï¼Œå‡å°‘é€‰å¸æ¬¡æ•°

    :param strategy:                ç­–ç•¥ï¼ŒåŒ…å«ï¼šå¤šå¤´é€‰å¸æ•°é‡ï¼Œç©ºå¤´é€‰å¸æ•°é‡ï¼Œåšå¤šå› å­åç§°ï¼Œåšç©ºå› å­åç§°ï¼Œå¤šå¤´èµ„é‡‘æƒé‡ï¼Œç©ºå¤´èµ„é‡‘æƒé‡
    :param long_df:                 å¤šå¤´é€‰å¸çš„df
    :param short_df:                ç©ºå¤´é€‰å¸çš„df
    :return:
    """
    """
    # åšå¤šé€‰å¸
    """
    if strategy.long_cap_weight > 0:
        long_df = calc_select_factor_rank(long_df, factor_column=strategy.long_factor, ascending=True)

        long_df = strategy.select_by_coin_num(long_df, strategy.long_select_coin_num, max_limit=strategy.long_select_coin_num_max)

        long_df['æ–¹å‘'] = 1
        long_df['target_alloc_ratio'] = 1 / long_df.groupby('candle_begin_time')['symbol'].transform('size')
    else:
        long_df = pd.DataFrame()

    """
    # åšç©ºé€‰å¸
    """
    if strategy.short_cap_weight > 0:
        short_df = calc_select_factor_rank(short_df, factor_column=strategy.short_factor, ascending=False)

        if strategy.short_select_coin_num == 'long_nums':  # å¦‚æœå‚æ•°æ˜¯long_numsï¼Œåˆ™ç©ºå¤´ä¸å¤šå¤´çš„é€‰å¸æ•°é‡ä¿æŒä¸€è‡´
            # è·å–åˆ°å¤šå¤´çš„é€‰å¸æ•°é‡å¹¶æ•´ç†æ•°æ®
            long_select_num = long_df.groupby('candle_begin_time')['symbol'].size().to_frame()
            long_select_num = long_select_num.rename(columns={'symbol': 'å¤šå¤´æ•°é‡'}).reset_index()
            # å°†å¤šå¤´é€‰å¸æ•°é‡æ•´ç†åˆ°short_df
            short_df = short_df.merge(long_select_num, on='candle_begin_time', how='left')
            # ä½¿ç”¨å¤šå¤´æ•°é‡å¯¹ç©ºå¤´æ•°æ®è¿›è¡Œé€‰å¸
            short_df = short_df[short_df['rank'] <= short_df['å¤šå¤´æ•°é‡']]
            del short_df['å¤šå¤´æ•°é‡']
        else:
            short_df = strategy.select_by_coin_num(short_df, strategy.short_select_coin_num, min_limit=strategy.short_select_coin_num_min)

        short_df['æ–¹å‘'] = -1
        short_df['target_alloc_ratio'] = 1 / short_df.groupby('candle_begin_time')['symbol'].transform('size')
    else:
        short_df = pd.DataFrame()

    # ===æ•´ç†æ•°æ®
    df = pd.concat([long_df, short_df], ignore_index=True)  # å°†åšå¤šå’Œåšç©ºçš„å¸ç§æ•°æ®åˆå¹¶
    df.sort_values(by=['candle_begin_time', 'æ–¹å‘'], ascending=[True, False], inplace=True)
    df.reset_index(drop=True, inplace=True)

    del df['æ€»å¸æ•°'], df['rank_max']

    return df


def select_coins_by_strategy(factor_df, stg_conf: StrategyConfig):
    """
    é’ˆå¯¹æ¯ä¸€ä¸ªç­–ç•¥ï¼Œè¿›è¡Œé€‰å¸ï¼Œå…·ä½“åˆ†ä¸ºä»¥ä¸‹4æ­¥ï¼š
    - 4.1 æ•°æ®æ¸…æ´—
    - 4.2 è®¡ç®—ç›®æ ‡é€‰å¸å› å­
    - 4.3 å‰ç½®è¿‡æ»¤ç­›é€‰
    - 4.4 æ ¹æ®é€‰å¸å› å­è¿›è¡Œé€‰å¸
    :param stg_conf: ç­–ç•¥é…ç½®
    :param factor_df: æ‰€æœ‰å¸ç§Kçº¿æ•°æ®ï¼Œä»…åŒ…å«éƒ¨åˆ†è¡Œæƒ…æ•°æ®å’Œé€‰å¸éœ€è¦çš„å› å­åˆ—
    :return: é€‰å¸æ•°æ®
    """

    """
    4.1 æ•°æ®é¢„å¤„ç†
    å¯ä»¥é¢„ç•™ä¸€äº›ç©ºé—´ç»™æ•°æ®æ•´ç†ï¼Œæ¯”å¦‚ç¼ºå¤±æ•°æ®çš„å¤„ç†
    """
    pass

    """
    4.2 è®¡ç®—ç›®æ ‡é€‰å¸å› å­
    - è®¡ç®—è¯¦æƒ…åœ¨ `strategy -> *.py`
    """
    s = time.time()
    # ç¼“å­˜è®¡ç®—å‰çš„åˆ—å
    prev_cols = factor_df.columns
    # è®¡ç®—å› å­
    result_df = stg_conf.calc_select_factor(factor_df)
    # åˆå¹¶æ–°çš„å› å­
    factor_df = factor_df[prev_cols].join(result_df[list(set(result_df.columns) - set(prev_cols))])
    logger.debug(f'[{stg_conf.name}] é€‰å¸å› å­è®¡ç®—è€—æ—¶ï¼š{time.time() - s:.2f}s')

    """
    4.3 å‰ç½®è¿‡æ»¤ç­›é€‰
    - è®¡ç®—è¯¦æƒ…åœ¨ `strategy -> *.py`
    """
    s = time.time()
    long_df, short_df = stg_conf.filter_before_select(factor_df)
    short_df = short_df[short_df['symbol_swap'] != '']  # ä¿ç•™æœ‰åˆçº¦çš„ç°è´§
    logger.debug(f'[{stg_conf.name}] å‰ç½®è¿‡æ»¤è€—æ—¶ï¼š{time.time() - s:.2f}s')

    """
    4.4 æ ¹æ®é€‰å¸å› å­è¿›è¡Œé€‰å¸
    """
    s = time.time()
    # å¤šå¤´é€‰å¸æ•°æ®ã€ç©ºå¤´é€‰å¸æ•°æ®ã€ç­–ç•¥é…ç½®
    factor_df = select_long_and_short_coin(stg_conf, long_df, short_df)
    logger.debug(f'[{stg_conf.name}] å¤šç©ºé€‰å¸è€—æ—¶ï¼š{time.time() - s:.2f}s')

    """
    4.5 åç½®è¿‡æ»¤ç­›é€‰
    """
    factor_df = stg_conf.filter_after_select(factor_df)
    logger.debug(f'[{stg_conf.name}] åç½®è¿‡æ»¤è€—æ—¶ï¼š{time.time() - s:.2f}s')

    """
    4.6 æ ¹æ®å¤šç©ºæ¯”è°ƒæ•´å¸ç§çš„æƒé‡
    """
    long_ratio = stg_conf.long_cap_weight / (stg_conf.long_cap_weight + stg_conf.short_cap_weight)
    factor_df.loc[factor_df['æ–¹å‘'] == 1, 'target_alloc_ratio'] = factor_df['target_alloc_ratio'] * long_ratio
    factor_df.loc[factor_df['æ–¹å‘'] == -1, 'target_alloc_ratio'] = factor_df['target_alloc_ratio'] * (1 - long_ratio)
    factor_df = factor_df[factor_df['target_alloc_ratio'].abs() > 1e-9]  # å»é™¤æƒé‡ä¸º0çš„æ•°æ®

    return factor_df[[*KLINE_COLS, 'æ–¹å‘', 'target_alloc_ratio']]


def process_strategy(stg_conf: StrategyConfig, result_folder: Path, is_silent=False, unified_time='2017-01-01', factor_df=None):
    import logging
    if is_silent:
        logger.setLevel(logging.WARNING)  # å¯ä»¥å‡å°‘ä¸­é—´è¾“å‡ºçš„log
    s = time.time()
    strategy_name = stg_conf.name
    logger.debug(f'[{stg_conf.name}] å¼€å§‹é€‰å¸...')

    # å‡†å¤‡é€‰å¸ç”¨æ•°æ® (V2 - L6 ä¼˜åŒ–: æå…¶é‡è¦ï¼æ­¤æ—¶ factor_df å·²ç»æ˜¯åˆå¹¶å¥½çš„ Master DataSet)
    # ç›´æ¥ä½¿ç”¨ï¼Œä¸å†è¿›è¡Œä»»ä½•ç£ç›˜è¯»å–æˆ– Join
    if factor_df is None:
        import polars as pl
        all_kline_pq = get_file_path(*ALL_KLINE_PATH_TUPLE, as_path_type=True).with_suffix('.parquet')
        factor_df = pl.read_parquet(all_kline_pq).to_pandas() if all_kline_pq.exists() else pd.DataFrame()

    factor_df = factor_df[factor_df['æ˜¯å¦äº¤æ˜“'] == 1]

    select_scope = stg_conf.select_scope
    order_first = stg_conf.order_first
    if select_scope == 'spot':
        condition = (factor_df['is_spot'] == 1)
    elif select_scope == 'swap':
        condition = (factor_df['is_spot'] == 0)
    else:  # mix æ··åˆ
        both_not_null = (factor_df['symbol_spot'] != '') & (factor_df['symbol_swap'] != '')
        # æ ¹æ®ä¼˜å…ˆä¸‹å•ï¼Œå¤„ç†é€‰å¸çš„å¸ç§
        order_first_symbol = (factor_df['is_spot'] == (1 if order_first == 'spot' else 0))
        condition = (~both_not_null | order_first_symbol)
    factor_df = factor_df.loc[condition, :].copy()

    factor_df.dropna(subset=stg_conf.factor_columns, inplace=True)
    factor_df.dropna(subset=['symbol'], how='any', inplace=True)

    factor_df.sort_values(by=['candle_begin_time', 'symbol'], inplace=True)
    factor_df.reset_index(drop=True, inplace=True)

    logger.debug(f'[{stg_conf.name}] é€‰å¸æ•°æ®å‡†å¤‡å®Œæˆï¼Œæ¶ˆè€—æ—¶é—´ï¼š{time.time() - s:.2f}s')

    result_df = select_coins_by_strategy(factor_df, stg_conf)
    # ç”¨äºç¼“å­˜é€‰å¸ç»“æœï¼Œå¦‚æœç»“æœä¸ºç©ºï¼Œä¹Ÿä¼šç”Ÿæˆå¯¹åº”çš„ï¼Œç©ºçš„pklæ–‡ä»¶
    stg_select_result = result_folder / f'{stg_conf.get_fullname(as_folder_name=True)}.pkl'

    if result_df.empty:
        pd.DataFrame(columns=SELECT_RES_COLS).to_pickle(stg_select_result)
        return

    del factor_df

    # ç­›é€‰åˆé€‚çš„offset
    cal_offset_base_seconds = 3600 * 24 if stg_conf.is_day_period else 3600
    reference_date = pd.to_datetime(unified_time)
    time_diff_seconds = (result_df['candle_begin_time'] - reference_date).dt.total_seconds()
    offset = (time_diff_seconds / cal_offset_base_seconds).mod(stg_conf.period_num).astype('int8')
    result_df['offset'] = ((offset + 1 + stg_conf.period_num) % stg_conf.period_num).astype('int8')
    result_df = result_df[result_df['offset'].isin(stg_conf.offset_list)]

    if result_df.empty:
        pd.DataFrame(columns=SELECT_RES_COLS).to_pickle(stg_select_result)
        logger.setLevel(logging.DEBUG)
        return

    # æ·»åŠ å…¶ä»–çš„ç›¸å…³é€‰å¸ä¿¡æ¯
    select_result_dict = dict()
    for kline_col in KLINE_COLS:
        select_result_dict[kline_col] = result_df[kline_col].values

    select_result_dict['æ–¹å‘'] = result_df['æ–¹å‘'].astype('int8').values
    select_result_dict['offset'] = result_df['offset'].astype('int8').values
    select_result_dict['target_alloc_ratio'] = result_df['target_alloc_ratio'].values
    select_result_df = pd.DataFrame(select_result_dict, copy=False)
    del result_df

    select_result_df['strategy'] = strategy_name
    select_result_df['strategy'] = pd.Categorical(select_result_df['strategy'])

    # æ ¹æ®ç­–ç•¥èµ„é‡‘æƒé‡ï¼Œè°ƒæ•´ç›®æ ‡åˆ†é…æ¯”ä¾‹
    select_result_df['cap_weight'] = np.float64(stg_conf.cap_weight)
    select_result_df['target_alloc_ratio'] = np.float64(
        select_result_df['target_alloc_ratio']
        * select_result_df['cap_weight']
        / len(stg_conf.offset_list)
        * select_result_df['æ–¹å‘']
    )
    select_result_df['order_first'] = order_first

    # ç¼“å­˜åˆ°æœ¬åœ°æ–‡ä»¶
    select_result_df[SELECT_RES_COLS].to_pickle(stg_select_result)

    logger.debug(f'[{strategy_name}] è€—æ—¶: {(time.time() - s):.2f}s')
    gc.collect()
    logger.setLevel(logging.DEBUG)


# é€‰å¸æ•°æ®æ•´ç† & é€‰å¸
def select_coin_with_conf(conf: BacktestConfig, multi_process=True, silent=True):
    """
    ** ç­–ç•¥é€‰å¸ **
    - is_use_spot: Trueçš„æ—¶å€™ï¼Œä½¿ç”¨ç°è´§æ•°æ®å’Œåˆçº¦æ•°æ®;
    - Falseçš„æ—¶å€™ï¼Œåªä½¿ç”¨åˆçº¦æ•°æ®ã€‚æ‰€ä»¥è¿™ä¸ªæƒ…å†µæ›´ç®€å•

    :param conf: å›æµ‹é…ç½®
    :param multi_process: æ˜¯å¦å¯ç”¨å¤šè¿›ç¨‹
    :param silent: æ˜¯å¦é™é»˜
    :return:
    """
    import logging
    if silent:
        logger.setLevel(logging.WARNING)  # å¯ä»¥å‡å°‘ä¸­é—´è¾“å‡ºçš„log
    # ====================================================================================================
    # 2.1 åˆå§‹åŒ–
    # ====================================================================================================
    result_folder = conf.get_result_folder()  # é€‰å¸ç»“æœæ–‡ä»¶å¤¹

    if not multi_process:
        for index, strategy in enumerate(conf.strategy_list):
            logger.debug(f'â„¹ï¸ [{index + 1}/{len(conf.strategy_list)}] {conf.name}')
            process_strategy(strategy, result_folder, False, conf.unified_time)
        return

    # å¤šè¿›ç¨‹æ¨¡å¼ -> V2 ThreadPool æ¨¡å¼ (é¿å… 3.4GB Pickle å¼€é”€)
    from concurrent.futures import ThreadPoolExecutor
    with ThreadPoolExecutor(max_workers=job_num) as executor:
        futures = [executor.submit(process_strategy, stg, result_folder, silent, conf.unified_time, getattr(conf, 'shared_factor_df', None)) for stg in conf.strategy_list]

        for future in tqdm(as_completed(futures), total=len(conf.strategy_list), desc=f'ğŸš€ {conf.name}'):
            try:
                future.result()
            except Exception as e:
                logger.exception(e)
                exit(1)
    logger.setLevel(logging.DEBUG)  # æ—¥å¿—ç»“æœæ¢å¤ä¸€ä¸‹


def select_coins(confs: BacktestConfig | List[BacktestConfig], multi_process=True, factor_df=None):
    if isinstance(confs, BacktestConfig):
        # å¦‚æœæ˜¯å•ä¾‹ï¼Œå°±ç›´æ¥è¿”å›åŸæ¥çš„ç»“æœ
        if factor_df is not None:
            confs.shared_factor_df = factor_df
        return select_coin_with_conf(confs, multi_process=multi_process)

    # å¦åˆ™å°±ç›´æ¥å¹¶è¡Œå›æµ‹
    is_multi = True  
    is_silent = True
    if factor_df is not None:
        for conf in confs:
            conf.shared_factor_df = factor_df

    from concurrent.futures import ThreadPoolExecutor
    with ThreadPoolExecutor(max_workers=job_num) as executor:
        futures = [executor.submit(select_coin_with_conf, conf, is_multi, is_silent) for conf in confs]
        for future in tqdm(as_completed(futures), total=len(confs), desc='é€‰å¸'):
            try:
                future.result()
            except Exception as e:
                logger.exception(e)
                exit(1)


# endregion

# ======================================================================================
# é€‰å¸ç»“æœèšåˆ
# ======================================================================================
# region é€‰å¸ç»“æœèšåˆ
def transfer_swap(select_coin, df_swap):
    """
    å°†ç°è´§ä¸­çš„æ•°æ®æ›¿æ¢æˆåˆçº¦æ•°æ®ï¼Œä¸»è¦æ›¿æ¢ï¼šclose
    :param select_coin:     é€‰å¸æ•°æ®
    :param df_swap:         åˆçº¦æ•°æ®
    :return:
    """
    trading_cols = ['symbol', 'is_spot', 'close', 'next_close']
    spot_line_index = select_coin[(select_coin['symbol_swap'] != '') & (select_coin['is_spot'] == 1)].index

    spot_select_coin = select_coin.loc[spot_line_index].copy()
    swap_select_coin = select_coin.loc[select_coin.index.difference(spot_line_index)].copy()
    # ['candle_begin_time', 'symbol_swap', 'strategy', 'cap_weight', 'æ–¹å‘', 'offset', 'target_alloc_ratio']
    spot_select_coin = pd.merge(
        spot_select_coin, df_swap[['candle_begin_time', *trading_cols]],
        left_on=['candle_begin_time', 'symbol_swap'], right_on=['candle_begin_time', 'symbol'],
        how='left', suffixes=('', '_2'))

    # mergeå®Œæˆä¹‹åï¼Œå¯èƒ½å› ä¸ºæœ‰äº›åˆçº¦æ•°æ®ä¸Šçº¿ä¸è¶…è¿‡æŒ‡å®šçš„æ—¶é—´ï¼ˆmin_kline_numï¼‰,é€ æˆåˆå¹¶å¼‚å¸¸ï¼Œéœ€è¦æŒ‰ç…§åŸç°è´§é€»è¾‘æ‰§è¡Œ
    failed_merge_select_coin = spot_select_coin[spot_select_coin['close_2'].isna()][select_coin.columns].copy()

    spot_select_coin = spot_select_coin.dropna(subset=['close_2'], how='any')
    spot_select_coin['is_spot_2'] = spot_select_coin['is_spot_2'].astype(np.int8)

    spot_select_coin.drop(columns=trading_cols, inplace=True)
    rename_dict = {f'{trading_col}_2': trading_col for trading_col in trading_cols}
    spot_select_coin.rename(columns=rename_dict, inplace=True)

    # å°†æ‹†åˆ†çš„é€‰å¸æ•°æ®ï¼Œåˆå¹¶å›å»
    select_coin = pd.concat([swap_select_coin, failed_merge_select_coin, spot_select_coin], axis=0)
    select_coin.sort_values(['candle_begin_time', 'æ–¹å‘'], inplace=True)

    return select_coin


def concat_select_results(conf: BacktestConfig) -> None:
    """
    èšåˆç­–ç•¥é€‰å¸ç»“æœï¼Œå½¢æˆç»¼åˆé€‰å¸ç»“æœ
    :param conf:
    :return:
    """
    # å¦‚æœæ˜¯çº¯å¤šå¤´ç°è´§æ¨¡å¼ï¼Œé‚£ä¹ˆå°±ä¸è½¬æ¢åˆçº¦æ•°æ®ï¼Œåªä¸‹ç°è´§å•
    all_select_result_df_list = []  # å­˜å‚¨æ¯ä¸€ä¸ªç­–ç•¥çš„é€‰å¸ç»“æœ
    result_folder = conf.get_result_folder()
    select_result_path = result_folder / 'é€‰å¸ç»“æœ.pkl'

    for strategy in conf.strategy_list:
        stg_select_result = result_folder / f'{strategy.get_fullname(as_folder_name=True)}.pkl'
        # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°±è·³è¿‡
        if not os.path.exists(stg_select_result):
            continue
        # å¦‚æœæ–‡ä»¶å­˜åœ¨ï¼Œå°±è¯»å–
        all_select_result_df_list.append(pd.read_pickle(stg_select_result))
        # åˆ é™¤è¯¥ç­–ç•¥çš„é€‰å¸ç»“æœï¼Œå¦‚æœè¦ä¿ç•™å¯ä»¥æ³¨é‡Š
        if not conf.is_reserved('strategy'):
            stg_select_result.unlink()

    # å¦‚æœæ²¡æœ‰ä»»ä½•ç­–ç•¥çš„é€‰å¸ç»“æœï¼Œå°±ç›´æ¥è¿”å›
    if not all_select_result_df_list:
        pd.DataFrame(columns=SELECT_RES_COLS).to_pickle(select_result_path)
        return

    # èšåˆé€‰å¸ç»“æœ
    all_select_result_df = pd.concat(all_select_result_df_list, ignore_index=True)
    del all_select_result_df_list
    gc.collect()

    all_stg_select_first_time = all_select_result_df.groupby('strategy')['candle_begin_time'].first().max()
    all_select_result_df = all_select_result_df[all_select_result_df['candle_begin_time'] >= all_stg_select_first_time]
    all_select_result_df.to_pickle(select_result_path)

    return all_select_result_df


def process_select_results(conf: BacktestConfig) -> pd.DataFrame:
    select_result_path = conf.get_result_folder() / 'é€‰å¸ç»“æœ.pkl'
    if not select_result_path.exists():
        logger.warning('æ²¡æœ‰ç”Ÿæˆé€‰å¸æ–‡ä»¶ï¼Œç›´æ¥è¿”å›')
        return pd.DataFrame(columns=SELECT_RES_COLS)
    all_select_result_df = pd.read_pickle(select_result_path)

    # ä¸æ˜¯çº¯å¤šï¼Œä¸”æ˜¯ç°è´§ç­–ç•¥
    # ç­›é€‰ä¸€ä¸‹é€‰å¸ç»“æœï¼Œåˆ¤æ–­å…¶ä¸­çš„ ä¼˜å…ˆä¸‹å•æ ‡è®°æ˜¯ä»€ä¹ˆ
    cond1 = all_select_result_df['order_first'] == 'swap'  # ä¼˜å…ˆä¸‹å•åˆçº¦
    cond2 = all_select_result_df['is_spot'] == 1  # å½“å‰å¸ç§æ˜¯ç°è´§
    if not all_select_result_df[cond1 & cond2].empty:
        all_kline_df = pd.read_pickle(get_file_path(*ALL_KLINE_PATH_TUPLE))
        # å°†å«æœ‰ç°è´§çš„å¸ç§ï¼Œæ›¿æ¢æ‰å…¶ä¸­closeä»·æ ¼
        df_swap = all_kline_df[(all_kline_df['is_spot'] == 0) & (all_kline_df['symbol_spot'] != '')]
        no_transfer_df = all_select_result_df[~(cond1 & cond2)]
        all_select_result_df = transfer_swap(all_select_result_df[cond1 & cond2], df_swap)
        all_select_result_df = pd.concat([no_transfer_df, all_select_result_df], ignore_index=True)

    # åˆ é™¤é€‰å¸æ–‡ä»¶ï¼Œå¦‚æœè¦ä¿ç•™å¯ä»¥æ³¨é‡Š
    if not conf.is_reserved('select'):
        select_result_path.unlink()

    return all_select_result_df


def to_ratio_pivot(df_select: pd.DataFrame, candle_begin_times, columns) -> pd.DataFrame:
    """ä½¿ç”¨ Polars ä¼˜åŒ–é€è§†è¡¨è½¬æ¢å’Œ Reindexï¼Œå‡å°‘ GIL é”ç«äº‰å’Œå†…å­˜å¼€é”€"""
    if df_select.empty:
        return pd.DataFrame(index=candle_begin_times, columns=[], dtype=float).fillna(0)
    
    import polars as pl
    # è½¬æ¢é€‰å¸ç»“æœåˆ° Polars
    pl_select = pl.from_pandas(df_select[['candle_begin_time', columns, 'target_alloc_ratio']])
    
    # é€è§†è¡¨è½¬æ¢
    # æ³¨æ„ï¼šPolars çš„ pivot éœ€è¦å…ˆæŒ‰ index æ’åºä»¥ä¿è¯ç»“æœä¸€è‡´æ€§
    pl_pivot = pl_select.pivot(
        on=columns,
        index='candle_begin_time',
        values='target_alloc_ratio',
        aggregate_function='sum'
    ).sort('candle_begin_time')
    
    # æ„å»ºå®Œæ•´çš„æ—¶é—´åºåˆ— DataFrame è¿›è¡Œå³è¿æ¥ (ç­‰ä»·äº Pandas reindex)
    pl_times = pl.DataFrame({'candle_begin_time': candle_begin_times})
    
    # ç¡®ä¿ Join é”®çš„ç²¾åº¦ä¸€è‡´ (us)ï¼Œé¿å… SchemaError
    pl_times = pl_times.with_columns(pl.col('candle_begin_time').cast(pl.Datetime('us')))
    pl_pivot = pl_pivot.with_columns(pl.col('candle_begin_time').cast(pl.Datetime('us')))
    
    pl_pivot = pl_times.join(pl_pivot, on='candle_begin_time', how='left').fill_null(0)
    
    # è½¬å› Pandas
    df_ratio = pl_pivot.to_pandas().set_index('candle_begin_time')
    return df_ratio


def trim_ratio_delists(df_ratio: pd.DataFrame, end_time: pd.Timestamp, market_dict: dict, trade_type: str):
    """
    ** åˆ é™¤è¦ä¸‹æ¶çš„å¸ **
    å½“å¸ç§å³å°†ä¸‹æ¶çš„æ—¶å€™ï¼ŒæŠŠåç»­çš„æŒä»“è°ƒæ•´ä¸º 0
    :param df_ratio: ä»“ä½æ¯”ä¾‹
    :param end_time: å›æµ‹ç»“æŸæ—¶é—´
    :param market_dict: æ‰€æœ‰å¸ç§çš„Kçº¿æ•°æ®
    :param trade_type: spot or swap
    :return: ä»“ä½è°ƒæ•´åçš„æ¯”ä¾‹
    """
    for symbol in df_ratio.columns:
        df_market = market_dict[symbol]
        if len(df_market) < 2:
            continue

        # æ²¡æœ‰ä¸‹æ¶
        last_end_time = df_market['candle_begin_time'].iloc[-1]
        if last_end_time >= end_time:
            continue

        second_last_end_time = df_market['candle_begin_time'].iloc[-2]
        if (df_ratio.loc[second_last_end_time:, symbol].abs() > 1e-8).any():
            logger.warning(f'{trade_type} {symbol} ä¸‹æ¶é€‰å¸æƒé‡ä¸ä¸º 0ï¼Œæ¸…é™¤ {second_last_end_time} ä¹‹åçš„æƒé‡')
            df_ratio.loc[second_last_end_time:, symbol] = 0

    return df_ratio


def agg_strategy_offsets(df_select: pd.DataFrame, stg_conf: StrategyConfig):
    """ä½¿ç”¨ Polars ä¼˜åŒ–å¤š offset æƒé‡èšåˆï¼Œå¤§å¹…æå‡å®½ç­–ç•¥æ€§èƒ½"""
    if df_select.empty:
        return pd.DataFrame(columns=['candle_begin_time', 'symbol', 'target_alloc_ratio'])
    
    import polars as pl
    
    # è½¬æ¢ä¸º Polars DataFrame
    pl_select = pl.from_pandas(df_select[['candle_begin_time', 'symbol', 'target_alloc_ratio']])
    
    # Step 1: æŒ‰ (candle_begin_time, symbol) èšåˆæƒé‡
    pl_agg = pl_select.group_by(['candle_begin_time', 'symbol']).agg(
        pl.col('target_alloc_ratio').sum()
    )
    
    # Step 2: æ„å»ºå®Œæ•´çš„æ—¶é—´åºåˆ—
    time_min = pl_agg['candle_begin_time'].min()
    time_max = pl_agg['candle_begin_time'].max()
    
    # è·å–æ‰€æœ‰å”¯ä¸€ symbol
    symbols = pl_agg['symbol'].unique().sort()
    
    # æ„å»ºå®Œæ•´æ—¶é—´èŒƒå›´ (ä½¿ç”¨ datetime_range æ”¯æŒå°æ—¶çº§é—´éš”)
    candle_times = pl.datetime_range(time_min, time_max, interval='1h', eager=True)
    
    # åˆ›å»º symbol Ã— time çš„ç¬›å¡å°”ç§¯ä½œä¸ºå®Œæ•´ç´¢å¼•
    pl_full_index = pl.DataFrame({'candle_begin_time': candle_times}).join(
        pl.DataFrame({'symbol': symbols}), how='cross'
    )
    
    # ç¡®ä¿ datetime ç²¾åº¦ä¸€è‡´ (Î¼s) ä»¥é¿å… SchemaError
    pl_agg = pl_agg.with_columns(pl.col('candle_begin_time').cast(pl.Datetime('us')))
    pl_full_index = pl_full_index.with_columns(pl.col('candle_begin_time').cast(pl.Datetime('us')))
    
def agg_strategy_offsets(pl_select: pl.DataFrame, stg_conf: StrategyConfig):
    """
    [L7 Zero-Copy Optimization] Polars-native agg_strategy_offsets
    Input: Polars DataFrame
    Output: Polars DataFrame
    """
    if pl_select.is_empty():
        return pl.DataFrame(schema={
            'candle_begin_time': pl.Datetime('us'),
            'symbol': pl.String,
            'target_alloc_ratio': pl.Float64
        })
    
    # Step 1: æŒ‰ (candle_begin_time, symbol) èšåˆæƒé‡
    pl_agg = pl_select.group_by(['candle_begin_time', 'symbol']).agg(
        pl.col('target_alloc_ratio').sum()
    )
    
    # Step 3: æŒ‰ symbol åˆ†ç»„ï¼Œå¯¹ target_alloc_ratio è¿›è¡Œ rolling sum
    # è§£æ hold_period (å¯èƒ½æ˜¯ '1H', '24H' ç­‰å­—ç¬¦ä¸²æ ¼å¼)
    hold_period_str = str(stg_conf.hold_period)
    if hold_period_str.endswith('H') or hold_period_str.endswith('h'):
        hold_period = int(hold_period_str[:-1])  # æå–æ•°å­—éƒ¨åˆ†
    else:
        hold_period = int(hold_period_str)  # ç›´æ¥è½¬æ¢
    
    # [ä¼˜åŒ–] å¦‚æœ hold_period ä¸º 1ï¼Œåˆ™ä¸éœ€è¦ rolling å’Œæ—¶é—´å¯¹é½ï¼Œç›´æ¥è¿”å›èšåˆç»“æœ
    # åªè¦åœ¨æœ€ç»ˆ pivot æ—¶è¡¥å…¨æ—¶é—´å³å¯ã€‚è¿™å¯¹äº S2 (å¤šå¤´å…¨å¸‚åœº) ç­‰å¯†é›†å‹ç­–ç•¥èƒ½å¸¦æ¥æå¤§åŠ é€Ÿ (é¿å… 44M è¡Œçš„ Sort + Rolling)
    if hold_period == 1:
        # å¼ºåˆ¶ symbol ä¸º String ç±»å‹ï¼Œä¸”ç¡®ä¿æ—¶é—´ç²¾åº¦ä¸º us
        return pl_agg.with_columns([
            pl.col('symbol').cast(pl.String),
            pl.col('candle_begin_time').cast(pl.Datetime('us'))
        ])

    # æ„å»ºå®Œæ•´æ—¶é—´èŒƒå›´ (ä½¿ç”¨ datetime_range æ”¯æŒå°æ—¶çº§é—´éš”)
    time_min = pl_agg['candle_begin_time'].min()
    time_max = pl_agg['candle_begin_time'].max()
    symbols = pl_agg['symbol'].unique().sort()
    
    candle_times = pl.datetime_range(time_min, time_max, interval='1h', eager=True)
    
    # åˆ›å»º symbol Ã— time çš„ç¬›å¡å°”ç§¯ä½œä¸ºå®Œæ•´ç´¢å¼•
    pl_full_index = pl.DataFrame({'candle_begin_time': candle_times}).join(
        pl.DataFrame({'symbol': symbols}), how='cross'
    )
    
    # ç¡®ä¿ datetime ç²¾åº¦ä¸€è‡´ (Î¼s) ä»¥é¿å… SchemaError
    pl_agg = pl_agg.with_columns(pl.col('candle_begin_time').cast(pl.Datetime('us')))
    pl_full_index = pl_full_index.with_columns(pl.col('candle_begin_time').cast(pl.Datetime('us')))
    
    # Left join å¾—åˆ°å®Œæ•´çš„ç¨€ç–çŸ©é˜µ
    pl_full = pl_full_index.join(pl_agg, on=['candle_begin_time', 'symbol'], how='left').fill_null(0)

    pl_result = pl_full.sort(['symbol', 'candle_begin_time']).with_columns(
        pl.col('target_alloc_ratio').rolling_sum(window_size=hold_period, min_periods=1).over('symbol')
    )
    
    # å¼ºåˆ¶ symbol ä¸º String ç±»å‹ï¼Œé¿å… concat æ—¶ String/Categorical ä¸ä¸€è‡´é”™è¯¯
    pl_result = pl_result.with_columns(pl.col('symbol').cast(pl.String))
    
    # è¿”å› Polars DataFrameï¼Œä¸è½¬ Pandasï¼
    return pl_result


def agg_multi_strategy_ratio(conf: BacktestConfig, df_select: pd.DataFrame):
    """
    èšåˆå¤šoffsetã€å¤šç­–ç•¥é€‰å¸ç»“æœä¸­çš„target_alloc_ratio
    :param conf: å›æµ‹é…ç½®
    :param df_select: é€‰å¸ç»“æœ
    :return: èšåˆåçš„df_spot_ratio å’Œ df_swap_ratioã€‚

    æ•°æ®ç»“æ„:
    - index_colä¸ºcandle_begin_timeï¼Œ
    - columnsä¸ºsymbolï¼Œ
    - valuesä¸ºtarget_alloc_ratioçš„èšåˆç»“æœ

    ç¤ºä¾‹:
                    1000BONK-USDT	1000BTTC-USDT	1000FLOKI-USDT	1000LUNC-USDT	1000PEPE-USDT	1000RATS-USDT	1000SATS-USDT	1000SHIB-USDT	1000XEC-USDT	1INCH-USDT	AAVE-USDT	ACE-USDT	ADA-USDT	    AEVO-USDT   ...
    2021/1/1 00:00	0	            0	            0	            0	            0	            0	            0	            0	            0	            0	        0	        0	        -0.083333333	0           ...
    2021/1/1 01:00	0	            0	            0	            0	            0	            0	            0	            0	            0	            0	        0	        0	        -0.083333333	0           ...
    2021/1/1 02:00	0	            0	            0	            0	            0	            0	            0	            0	            0	            0	        0	        0	        -0.083333333	0           ...
    2021/1/1 03:00	0	            0	            0	            0	            0	            0	            0	            0	            0	            0	        0	        0	        -0.083333333	0           ...
    2021/1/1 04:00	0	            0	            0	            0	            0	            0	            0	            0	            0	            0	        0	        0	        -0.083333333	0           ...
    2021/1/1 05:00	0	            0	            0	            0	            0	            0	            0	            0	            0	            0	        0	        0	        -0.083333333	0           ...
    2021/1/1 06:00	0	            0	            0	            0	            0	            0	            0	            0	            0	            0	        0	        0	        -0.083333333	0           ...
    2021/1/1 07:00	0	            0	            0	            0	            0	            0	            0	            0	            0	            0	        0	        0	        -0.083333333	0           ...
    2021/1/1 08:00	0	            0	            0	            0	            0	            0	            0	            0	            0	            0	        0	        0	        -0.083333333	0           ...
    2021/1/1 09:00	0	            0	            0	            0	            0	            0	            0	            0	            0	            0	        0	        0	        -0.083333333	0           ...
    """
    # ====================================================================================================
    # 1. å…ˆé’ˆå¯¹æ¯ä¸ªç­–ç•¥çš„å¤šoffsetè¿›è¡Œèšåˆ
    # ====================================================================================================
    df_spot_select_list = []
    df_swap_select_list = []

    # å¦‚æœæ˜¯Dçš„æŒä»“å‘¨æœŸï¼Œåº”è¯¥æ˜¯å½“å¤©çš„é€‰å¸ï¼Œç¬¬äºŒå¤©0ç‚¹æŒä»“ã€‚
    # æŒ‰ç…§ç›®å‰çš„é€»è¾‘ï¼ŒåŸæ¥è‡ªå¸¦çš„begin timeæ˜¯0ç‚¹
    if conf.is_day_period:
        df_select['candle_begin_time'] = df_select['candle_begin_time'] + pd.Timedelta(hours=23)

    for strategy in conf.strategy_list:
        # è£åˆ‡å½“å‰ç­–ç•¥çš„spoté€‰å¸ç»“æœ
        df_select_spot = df_select[(df_select['strategy'] == strategy.name) & (df_select['is_spot'] == 1)]
        # ä¹°å…¥ç°è´§éƒ¨åˆ†
        _spot_select_long = agg_strategy_offsets(df_select_spot[df_select_spot['æ–¹å‘'] == 1], strategy)
        df_spot_select_list.append(_spot_select_long)
        # åšç©ºç°è´§éƒ¨åˆ†
        _spot_select_short = agg_strategy_offsets(df_select_spot[df_select_spot['æ–¹å‘'] == -1], strategy)
        df_spot_select_list.append(_spot_select_short)

        # è£åˆ‡å½“å‰ç­–ç•¥çš„swapé€‰å¸ç»“æœ
        df_select_swap = df_select[(df_select['strategy'] == strategy.name) & (df_select['is_spot'] == 0)]
        # ä¹°å…¥åˆçº¦éƒ¨åˆ†
        _swap_select_long = agg_strategy_offsets(df_select_swap[df_select_swap['æ–¹å‘'] == 1], strategy)
        df_swap_select_list.append(_swap_select_long)
        # åšç©ºåˆçº¦éƒ¨åˆ†
        _swap_select_short = agg_strategy_offsets(df_select_swap[df_select_swap['æ–¹å‘'] == -1], strategy)
        df_swap_select_list.append(_swap_select_short)

def agg_multi_strategy_ratio(conf: BacktestConfig, df_select: pd.DataFrame):
    """
    [L7 Zero-Copy Optimization] Polars-native Aggregation Pipeline
    """
    import polars as pl
    
    # 1. ç«‹å³è½¬æ¢ä¸º Polarsï¼Œåç»­å…¨ç¨‹ Zero-Copy
    pl_select = pl.from_pandas(df_select)
    
    # å¦‚æœæ˜¯Dçš„æŒä»“å‘¨æœŸï¼Œè°ƒæ•´æ—¶é—´
    if conf.is_day_period:
        pl_select = pl_select.with_columns(
            (pl.col('candle_begin_time') + pl.duration(hours=23)).alias('candle_begin_time')
        )

    pl_spot_list = []
    pl_swap_list = []

    for strategy in conf.strategy_list:
        # ä½¿ç”¨ Polars è¿‡æ»¤ï¼Œæå¤§æå‡é€Ÿåº¦
        # 1. Spot è¿‡æ»¤
        pl_stg_spot = pl_select.filter((pl.col('strategy') == strategy.name) & (pl.col('is_spot') == 1))
        if len(pl_stg_spot) > 0:
            pl_spot_list.append(agg_strategy_offsets(pl_stg_spot.filter(pl.col('æ–¹å‘') == 1), strategy))
            pl_spot_list.append(agg_strategy_offsets(pl_stg_spot.filter(pl.col('æ–¹å‘') == -1), strategy))

        # 2. Swap è¿‡æ»¤
        pl_stg_swap = pl_select.filter((pl.col('strategy') == strategy.name) & (pl.col('is_spot') == 0))
        if len(pl_stg_swap) > 0:
            pl_swap_list.append(agg_strategy_offsets(pl_stg_swap.filter(pl.col('æ–¹å‘') == 1), strategy))
            pl_swap_list.append(agg_strategy_offsets(pl_stg_swap.filter(pl.col('æ–¹å‘') == -1), strategy))

    # ä½¿ç”¨ Polars Concatï¼Œä¸éœ€è¦ reindex
    pl_spot_agg = pl.concat(pl_spot_list) if pl_spot_list else pl.DataFrame()
    pl_swap_agg = pl.concat(pl_swap_list) if pl_swap_list else pl.DataFrame()

    # ====================================================================================================
    # 2. é’ˆå¯¹å¤šç­–ç•¥è¿›è¡Œèšåˆ (Polars Pivot)
    # ====================================================================================================
    candle_begin_times = pd.date_range(conf.start_date, conf.end_date, freq='H', inclusive='left')

    # å°† Polars DataFrame ç›´æ¥ä¼ ç»™ pivot å‡½æ•° (éœ€ç¡®ä¿ to_ratio_pivot æ”¯æŒ Polars æˆ–åœ¨æ­¤å¤„ç†)
    # æˆ‘ä»¬å¯ä»¥ç¨å¾®ä¿®æ”¹é€»è¾‘ï¼Œç›´æ¥åœ¨è¿™é‡Œåšæœ€ç»ˆ Pivotï¼Œæˆ–è®© to_ratio_pivot å…¼å®¹
    
    # è¿™é‡Œç›´æ¥åœ¨ Polars å†…éƒ¨åš Pivotï¼Œæ•ˆç‡æœ€é«˜
    def _polars_pivot_to_pandas(pl_df, times):
        if pl_df.is_empty():
            return pd.DataFrame(index=times, columns=[], dtype=float).fillna(0)
        
        # æŒ‰ candle_begin_time å’Œ symbol å†æ¬¡èšåˆ (åˆå¹¶å¤šç­–ç•¥)
        pl_grouped = pl_df.group_by(['candle_begin_time', 'symbol']).agg(
            pl.col('target_alloc_ratio').sum()
        )
        
        # Pivot
        pl_pivoted = pl_grouped.pivot(
            on='symbol',
            index='candle_begin_time',
            values='target_alloc_ratio',
            aggregate_function='sum'
        ).sort('candle_begin_time')
        
        # å¯¹é½æ—¶é—´ (Right Join)
        pl_times = pl.DataFrame({'candle_begin_time': times})
        pl_times = pl_times.with_columns(pl.col('candle_begin_time').cast(pl.Datetime('us')))
        # ç¡®ä¿ pl_pivoted æ—¶é—´åˆ—ä¹Ÿæ˜¯ us
        pl_pivoted = pl_pivoted.with_columns(pl.col('candle_begin_time').cast(pl.Datetime('us')))
        
        pl_final = pl_times.join(pl_pivoted, on='candle_begin_time', how='left').fill_null(0)
        return pl_final.to_pandas().set_index('candle_begin_time')

    df_spot_ratio = _polars_pivot_to_pandas(pl_spot_agg, candle_begin_times)
    df_swap_ratio = _polars_pivot_to_pandas(pl_swap_agg, candle_begin_times)

    # # é’ˆå¯¹ä¸‹æ¶å¸çš„å¤„ç†
    # df_spot_ratio = trim_ratio_delists(df_spot_ratio, candle_begin_times.max(), spot_dict, 'spot')
    # df_swap_ratio = trim_ratio_delists(df_swap_ratio, candle_begin_times.max(), swap_dict, 'swap')

    return df_spot_ratio, df_swap_ratio
