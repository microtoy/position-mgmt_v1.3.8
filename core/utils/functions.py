"""
é‚¢ä¸è¡Œï½œç­–ç•¥åˆ†äº«ä¼š
ä»“ä½ç®¡ç†æ¡†æ¶

ç‰ˆæƒæ‰€æœ‰ Â©ï¸ é‚¢ä¸è¡Œ
å¾®ä¿¡: xbx1717

æœ¬ä»£ç ä»…ä¾›ä¸ªäººå­¦ä¹ ä½¿ç”¨ï¼Œæœªç»æˆæƒä¸å¾—å¤åˆ¶ã€ä¿®æ”¹æˆ–ç”¨äºå•†ä¸šç”¨é€”ã€‚

Author: é‚¢ä¸è¡Œ
"""

import gc
import shutil
import warnings
from pathlib import Path
from typing import Dict, List

import numpy as np
import pandas as pd
import polars as pl

from config import stable_symbol, swap_path, spot_path
from core.model.backtest_config import BacktestConfig
from core.utils.log_kit import logger
from core.utils.path_kit import get_file_path

warnings.filterwarnings('ignore')


# =====ç­–ç•¥ç›¸å…³å‡½æ•°
def del_insufficient_data(symbol_candle_data) -> Dict[str, pd.DataFrame]:
    """
    åˆ é™¤æ•°æ®é•¿åº¦ä¸è¶³çš„å¸ç§ä¿¡æ¯

    :param symbol_candle_data:
    :return
    """
    # ===åˆ é™¤æˆäº¤é‡ä¸º0çš„çº¿æ•°æ®ã€kçº¿æ•°ä¸è¶³çš„å¸ç§
    symbol_list = list(symbol_candle_data.keys())
    for symbol in symbol_list:
        # åˆ é™¤ç©ºçš„æ•°æ®
        if symbol_candle_data[symbol] is None or symbol_candle_data[symbol].empty:
            del symbol_candle_data[symbol]
            continue
        # åˆ é™¤è¯¥å¸ç§æˆäº¤é‡=0çš„kçº¿
        # symbol_candle_data[symbol] = symbol_candle_data[symbol][symbol_candle_data[symbol]['volume'] > 0]

    return symbol_candle_data


def ignore_error(anything):
    return anything


def load_min_qty(file_path: Path) -> (int, Dict[str, int]):
    # è¯»å–min_qtyæ–‡ä»¶å¹¶è½¬ä¸ºdictæ ¼å¼
    min_qty_df = pd.read_csv(file_path, encoding='utf-8-sig')
    min_qty_df['æœ€å°ä¸‹å•é‡'] = -np.log10(min_qty_df['æœ€å°ä¸‹å•é‡']).round().astype(int)
    default_min_qty = min_qty_df['æœ€å°ä¸‹å•é‡'].max()
    min_qty_df.set_index('å¸ç§', inplace=True)
    min_qty_dict = min_qty_df['æœ€å°ä¸‹å•é‡'].to_dict()

    return default_min_qty, min_qty_dict


def is_trade_symbol(symbol, black_list, white_list) -> bool:
    """
    è¿‡æ»¤æ‰ä¸èƒ½ç”¨äºäº¤æ˜“çš„å¸ç§ï¼Œæ¯”å¦‚ç¨³å®šå¸ã€éUSDTäº¤æ˜“å¯¹ï¼Œä»¥åŠä¸€äº›æ æ†å¸
    :param symbol: äº¤æ˜“å¯¹
    :param black_list: é»‘åå•
    :param white_list: ç™½åå•
    :return: æ˜¯å¦å¯ä»¥è¿›å…¥äº¤æ˜“ï¼ŒTrueå¯ä»¥å‚ä¸é€‰å¸ï¼ŒFalseä¸å‚ä¸
    """
    symbol = symbol.upper().replace('-USDT', 'USDT')
    if white_list:
        if symbol in white_list:
            return True
        else:
            return False

    # ç¨³å®šå¸å’Œé»‘åå•å¸ä¸å‚ä¸
    if not symbol or not symbol.endswith('USDT') or symbol in black_list:
        return False

    # ç­›é€‰æ æ†å¸
    base_symbol = symbol[:-4]
    if base_symbol.endswith(('UP', 'DOWN', 'BEAR', 'BULL')) and base_symbol != 'JUP' and base_symbol != 'SYRUP' or base_symbol in stable_symbol:
        return False
    else:
        return True


def align_spot_swap_mapping(df, column_name, n):
    """
    å¤„ç†spotå’Œswapçš„æ˜ å°„å…³ç³»
    :param df: åŸå§‹kçº¿æ•°æ®
    :param column_name: éœ€è¦å¤„ç†çš„åˆ—
    :param n: éœ€è¦è°ƒæ•´æ˜ å°„çš„å‘¨æœŸæ•°é‡
    :return: è°ƒæ•´å¥½çš„kçº¿æ•°æ®
    """
    # åˆ›å»ºæ–°ç»„æ ‡è¯†åˆ—
    df['is_new_group'] = (df[column_name].ne('') & df[column_name].shift().eq('')).astype(int)
    # ç´¯ç§¯æ±‚å’Œç”Ÿæˆç»„å·
    df['group'] = df['is_new_group'].cumsum()
    # å°†ç©ºå­—ç¬¦ä¸²å¯¹åº”çš„ç»„å·è®¾ä¸ºNaN
    df.loc[df['symbol_swap'].eq(''), 'group'] = np.nan
    # é€šè¿‡ groupby æ·»åŠ  grp_seq
    df['grp_seq'] = df.groupby('group').cumcount()
    # è¿‡æ»¤æ¡ä»¶å¹¶ä¿®æ”¹å‰ n è¡Œ
    df.loc[df['grp_seq'] < n, column_name] = ''

    # åˆ é™¤è¾…åŠ©åˆ—
    df.drop(columns=['is_new_group', 'group', 'grp_seq'], inplace=True)

    return df


def pl_is_trade_symbol(symbol_series, black_list):
    """Polars ç‰ˆæœ¬çš„å¸ç§è¿‡æ»¤"""
    # ç»Ÿä¸€æ ¼å¼
    symbols = symbol_series.str.to_uppercase().str.replace("-USDT", "USDT")
    
    # åŸºç¡€è¿‡æ»¤ï¼šå¿…é¡»ä»¥ USDT ç»“å°¾
    mask = symbols.str.ends_with("USDT")
    
    # é»‘åå•è¿‡æ»¤
    if black_list:
        mask = mask & (~symbols.is_in(black_list))
    
    # æå– base_symbol (V2 ä¿®æ­£: Polars slice ç¬¬äºŒä¸ªå‚æ•°æ˜¯é•¿åº¦ï¼Œä¸èƒ½ä¸ºè´Ÿ)
    base_symbols = symbols.str.slice(0, symbols.str.len_chars() - 4)
    
    # æ æ†å¸è¿‡æ»¤ (UP/DOWN/BEAR/BULL)
    leverage_mask = (
        base_symbols.str.ends_with("UP") | 
        base_symbols.str.ends_with("DOWN") | 
        base_symbols.str.ends_with("BEAR") | 
        base_symbols.str.ends_with("BULL")
    ) & (base_symbols != "JUP") & (base_symbols != "SYRUP")
    
    mask = mask & (~leverage_mask)
    
    # ç¨³å®šå¸è¿‡æ»¤
    if stable_symbol:
        mask = mask & (~base_symbols.is_in(stable_symbol))
        
    return mask


def pl_align_spot_swap_mapping(df, column_name, n):
    """Polars ç‰ˆæœ¬çš„ spot/swap æ˜ å°„å¯¹é½"""
    col = pl.col(column_name)
    is_not_empty = col != ""
    is_prev_empty = col.shift(1).fill_null("") == ""
    is_new_group = (is_not_empty & is_prev_empty).cast(pl.Int32)
    
    # ç´¯ç§¯æ±‚å’Œç”Ÿæˆç»„å·
    group_ids = is_new_group.cum_sum()
    
    # åªæœ‰åœ¨è¯¥åˆ—éç©ºæ—¶æ‰æœ‰ç»„å·
    group_ids = pl.when(is_not_empty).then(group_ids).otherwise(None)
    
    # è®¡ç®—ç»„å†…åºå·å¹¶ç½®ç©ºå‰ n è¡Œ
    return df.with_columns([
        pl.when(
            (pl.int_range(0, pl.len()).over(group_ids) < n) & is_not_empty
        ).then(pl.lit("")).otherwise(col).alias(column_name)
    ])


def load_spot_and_swap_data(conf: BacktestConfig) -> (pd.DataFrame, pd.DataFrame):
    """
    åŠ è½½ç°è´§å’Œåˆçº¦æ•°æ® (Polars V2 ä¼˜åŒ–ç‰ˆ)
    :param conf: å›æµ‹é…ç½®
    :return:
    """
    cache_path = get_file_path('data', 'cache', as_path_type=True)
    cache_path.mkdir(parents=True, exist_ok=True)
    
    combined_pq = cache_path / "all_candle_data.parquet"
    combined_pkl = cache_path / "all_candle_df_list.pkl"

    # [V2 ä¼˜åŒ–] ç¼“å­˜ä¸€è‡´æ€§æ£€æŸ¥ï¼šå¦‚æœç¼“å­˜å­˜åœ¨ï¼Œè·³è¿‡æ‰«æ
    if combined_pq.exists() and combined_pkl.exists():
        logger.ok("ğŸš€ å‘ç°ç°æœ‰è¡Œæƒ…æ•°æ®ç¼“å­˜ï¼Œè·³è¿‡æ‰«æé˜¶æ®µã€‚")
        return # ç›´æ¥è¿”å›ï¼Œåç»­æµç¨‹ä¼šé€šè¿‡ select_coin.py åŠ è½½è¿™ä¸ªæ–‡ä»¶

    logger.debug('ğŸ’¿ åŠ è½½ç°è´§å’Œåˆçº¦æ•°æ® (Parquet Zero-Copy)...')
    
    # å…¼å®¹æ€§å¤„ç†ï¼šå°è¯•ä» config å¯¼å…¥ä¸åŒçš„è·¯å¾„å˜é‡
    import config
    if hasattr(config, 'fuel_data_path'):
        parquet_base = Path(config.fuel_data_path) / "coin-binance-spot-swap-preprocess-pkl-1h"
    elif hasattr(config, 'pre_data_path'):
        parquet_base = Path(config.pre_data_path)
    elif hasattr(config, 'raw_data_path'):
        parquet_base = Path(config.raw_data_path)
    else:
        raise ImportError("æ— æ³•åœ¨ config.py ä¸­æ‰¾åˆ°æ•°æ®è·¯å¾„é…ç½® (fuel_data_path æˆ– pre_data_path)")

    spot_pq = parquet_base / "spot.parquet"
    swap_pq = parquet_base / "swap.parquet"

    all_dfs = []
    all_symbols = set()

    # 1. åŠ è½½åˆçº¦æ•°æ®
    if not {'swap', 'mix'}.isdisjoint(conf.select_scope_set) or not {'swap'}.isdisjoint(conf.order_first_set):
        if swap_pq.exists():
            df = pl.read_parquet(swap_pq)
            # è¿‡æ»¤ä¸å¯äº¤æ˜“å¸ç§
            mask = pl_is_trade_symbol(df["symbol"], conf.black_list)
            df = df.filter(mask)
            
            # å¯¹é½æ˜ å°„ (æŒ‰å¸ç§åˆ†ç»„å¤„ç†)
            df = df.sort(["symbol", "candle_begin_time"])
            df = df.group_by("symbol", maintain_order=True).map_groups(
                lambda g: pl_align_spot_swap_mapping(g, 'symbol_spot', conf.min_kline_num)
            )
            
            # ç±»å‹è½¬æ¢ä»¥ç¡®ä¿ Schema ä¸€è‡´æ€§ (V2 ä¿®æ­£: è§£å†³ Int64 vs Float64 æŠ¥é”™)
            num_cols = ["open", "high", "low", "close", "volume", "quote_volume", "trade_num", 
                        "taker_buy_base_asset_volume", "taker_buy_quote_asset_volume", 
                        "funding_fee", "avg_price_1m", "avg_price_5m"]
            # è¿‡æ»¤ä¸å­˜åœ¨çš„åˆ—ä»¥é˜²æŠ¥é”™
            actual_num_cols = [c for c in num_cols if c in df.columns]
            df = df.with_columns([pl.col(c).cast(pl.Float64) for c in actual_num_cols])
            
            all_symbols.update(df["symbol"].unique().to_list())
            all_dfs.append(df)
            logger.debug(f"å·²åŠ è½½åˆçº¦æ•°æ®: {len(df['symbol'].unique())} å¸ç§")

    # 2. åŠ è½½ç°è´§æ•°æ®
    if not {'spot', 'mix'}.isdisjoint(conf.select_scope_set):
        if spot_pq.exists():
            df = pl.read_parquet(spot_pq)
            # è¿‡æ»¤
            mask = pl_is_trade_symbol(df["symbol"], conf.black_list)
            df = df.filter(mask)
            
            # å¯¹é½
            df = df.sort(["symbol", "candle_begin_time"])
            df = df.group_by("symbol", maintain_order=True).map_groups(
                lambda g: pl_align_spot_swap_mapping(g, 'symbol_swap', conf.min_kline_num)
            )
            
            # ç±»å‹è½¬æ¢ä»¥ç¡®ä¿ Schema ä¸€è‡´æ€§
            actual_num_cols = [c for c in num_cols if c in df.columns]
            df = df.with_columns([pl.col(c).cast(pl.Float64) for c in actual_num_cols])
            
            spot_symbols = df["symbol"].unique().to_list()
            all_symbols.update(spot_symbols)
            all_dfs.append(df)
            logger.debug(f"å·²åŠ è½½ç°è´§æ•°æ®: {len(df['symbol'].unique())} å¸ç§")

    # 3. åˆå¹¶å¹¶ä¿å­˜ä¸ºä¸­é—´æ ¼å¼
    if all_dfs:
        full_df = pl.concat(all_dfs)
        # å…¼å®¹æ€§å¤„ç†ï¼šå­˜ä¸ºå• Parquet æ–‡ä»¶ç”¨äºåç»­ä¼˜åŒ–ï¼ŒåŒæ—¶ç”Ÿæˆ pickle list ä»¥é˜²ä¸‡ä¸€
        combined_pq = cache_path / "all_candle_data.parquet"
        full_df.write_parquet(combined_pq)
        
        # æš‚æ—¶ä¿ç•™ pickle list å…¼å®¹ç°æœ‰ä»£ç 
        needed_cols = ["candle_begin_time", "symbol", "open", "high", "low", "close", "volume", 
                       "quote_volume", "trade_num", "taker_buy_base_asset_volume", 
                       "taker_buy_quote_asset_volume", "funding_fee", "avg_price_1m", 
                       "avg_price_5m", "æ˜¯å¦äº¤æ˜“", "first_candle_time", "last_candle_time", 
                       "symbol_spot", "symbol_swap", "is_spot"]
        
        logger.debug("æ­£åœ¨ç”Ÿæˆå…¼å®¹æ€§æ•°æ®ç¼“å­˜ (all_candle_df_list.pkl)...")
        # è½¬æ¢ä¸º pandas å¹¶åˆ†ç»„
        pd_df = full_df.select(needed_cols).to_pandas()
        candle_df_list = [group for _, group in pd_df.groupby("symbol")]
        pd.to_pickle(candle_df_list, cache_path / "all_candle_df_list.pkl")
        
        del full_df, pd_df, candle_df_list
    
    gc.collect()
    return tuple(list(all_symbols))


def save_performance_df_csv(conf: BacktestConfig, **kwargs):
    # logger.debug(f'ğŸ’¾ ä¿å­˜å›æµ‹ç»“æœåˆ°æ–‡ä»¶å¤¹: {conf.get_result_folder()}')
    for name, df in kwargs.items():
        file_path = conf.get_result_folder() / f'{name}.csv'
        df.to_csv(file_path, encoding='utf-8-sig')


# ===============================================================================================================
# é¢å¤–æ•°æ®æº
# ===============================================================================================================
def merge_data(df: pd.DataFrame, data_name: str, save_cols: List[str], symbol: str = '') -> dict[str, pd.Series]:
    """
    å¯¼å…¥æ•°æ®ï¼Œæœ€ç»ˆåªè¿”å›å¸¦æœ‰åŒindexçš„æ•°æ®
    :param df: ï¼ˆåªè¯»ï¼‰åŸå§‹çš„è¡Œæƒ…æ•°æ®ï¼Œä¸»è¦æ˜¯å¯¹é½æ•°æ®ç”¨çš„
    :param data_name: æ•°æ®ä¸­å¿ƒä¸­çš„æ•°æ®è‹±æ–‡å
    :param save_cols: éœ€è¦ä¿å­˜çš„åˆ—
    :param symbol: å¸ç§
    :return: åˆå¹¶åçš„æ•°æ®
    """
    import core.data_bridge as db
    from config import data_source_dict

    func_name, file_path = data_source_dict[data_name]

    if hasattr(db, func_name):
        extra_df: pd.DataFrame = getattr(db, func_name)(file_path, df, save_cols, symbol)
    else:
        print(f'âš ï¸ æœªå®ç°æ•°æ®æºï¼š{data_name}')
        return {col: pd.Series([np.nan] * len(df)) for col in save_cols}

    if extra_df is None or extra_df.empty:
        return {col: pd.Series([np.nan] * len(df)) for col in save_cols}

    return {col: extra_df[col] for col in save_cols}


def check_cfg():
    """
    æ£€æŸ¥ data_source_dict é…ç½®
    æ£€æŸ¥åŠ è½½æ•°æ®æºå‡½æ•°æ˜¯å¦å­˜åœ¨
    æ£€æŸ¥æ•°æ®æºæ–‡ä»¶æ˜¯å¦å­˜åœ¨
    :return:
    """
    import core.data_bridge as db
    from config import data_source_dict
    for key, value in data_source_dict.items():
        func_name, file_path = value
        if not hasattr(db, func_name):
            raise Exception(f"ã€{key}ã€‘åŠ è½½æ•°æ®æºæ–¹æ³•æœªå®ç°ï¼š{func_name}")

        if not (file_path and Path(file_path).exists()):
            raise Exception(f"ã€{key}ã€‘æ•°æ®æºæ–‡ä»¶ä¸å­˜åœ¨ï¼š{file_path}")

    print('âœ… data_source_dict é…ç½®æ£€æŸ¥é€šè¿‡')


def check_factor(factors: list):
    """
    æ£€æŸ¥å› å­ä¸­çš„é…ç½®
    æ£€æŸ¥æ˜¯å¦æœ‰ extra_data_dict
    æ£€æŸ¥ extra_data_dict ä¸­çš„æ•°æ®æºæ˜¯å¦åœ¨ data_source_dict ä¸­

    å› å­ä¸­çš„å¤–éƒ¨æ•°æ®ä½¿ç”¨æ¡ˆä¾‹:

    extra_data_dict = {
        'coin-cap': ['circulating_supply']
    }

    :param factors:
    :return:
    """
    from core.utils.factor_hub import FactorHub
    for factor_name in factors:
        factor = FactorHub.get_by_name(factor_name)  # è·å–å› å­ä¿¡æ¯
        if not (hasattr(factor, 'extra_data_dict') and factor.extra_data_dict):
            raise Exception(f"æœªæ‰¾åˆ°ã€{factor_name}ã€‘å› å­ä¸­ extra_data_dict é…ç½®")

        for data_source in factor.extra_data_dict.keys():
            from config import data_source_dict
            if data_source not in data_source_dict:
                raise Exception(f"æœªæ‰¾åˆ° extra_data_dict é…ç½®çš„æ•°æ®æºï¼š{data_source}")

    print(f'âœ… {factors} å› å­é…ç½®æ£€æŸ¥é€šè¿‡')
